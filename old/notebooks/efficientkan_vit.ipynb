{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T14:07:44.484243Z",
     "iopub.status.busy": "2024-07-26T14:07:44.483401Z",
     "iopub.status.idle": "2024-07-26T14:07:44.488497Z",
     "shell.execute_reply": "2024-07-26T14:07:44.487549Z",
     "shell.execute_reply.started": "2024-07-26T14:07:44.484212Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Original code has been adapted from the KAN Paper.\n",
    "import math\n",
    "import numpy\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T14:07:44.497406Z",
     "iopub.status.busy": "2024-07-26T14:07:44.496733Z",
     "iopub.status.idle": "2024-07-26T14:07:44.504401Z",
     "shell.execute_reply": "2024-07-26T14:07:44.503397Z",
     "shell.execute_reply.started": "2024-07-26T14:07:44.497379Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "numpy.random.seed(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T14:07:44.697002Z",
     "iopub.status.busy": "2024-07-26T14:07:44.696666Z",
     "iopub.status.idle": "2024-07-26T14:07:44.732135Z",
     "shell.execute_reply": "2024-07-26T14:07:44.731301Z",
     "shell.execute_reply.started": "2024-07-26T14:07:44.696979Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class KANLinear(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        out_features,\n",
    "        grid_size=5,\n",
    "        spline_order=3,\n",
    "        scale_noise=0.1,\n",
    "        scale_base=1.0,\n",
    "        scale_spline=1.0,\n",
    "        enable_standalone_scale_spline=True,\n",
    "        base_activation=torch.nn.SiLU,\n",
    "        grid_eps=0.02,\n",
    "        grid_range=[-1, 1],\n",
    "    ):\n",
    "        super(KANLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.grid_size = grid_size\n",
    "        self.spline_order = spline_order\n",
    "\n",
    "        h = (grid_range[1] - grid_range[0]) / grid_size\n",
    "        grid = (\n",
    "            (\n",
    "                torch.arange(-spline_order, grid_size + spline_order + 1) * h\n",
    "                + grid_range[0]\n",
    "            )\n",
    "            .expand(in_features, -1)\n",
    "            .contiguous()\n",
    "        )\n",
    "        self.register_buffer(\"grid\", grid)\n",
    "\n",
    "        self.base_weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.spline_weight = torch.nn.Parameter(\n",
    "            torch.Tensor(out_features, in_features, grid_size + spline_order)\n",
    "        )\n",
    "        if enable_standalone_scale_spline:\n",
    "            self.spline_scaler = torch.nn.Parameter(\n",
    "                torch.Tensor(out_features, in_features)\n",
    "            )\n",
    "\n",
    "        self.scale_noise = scale_noise\n",
    "        self.scale_base = scale_base\n",
    "        self.scale_spline = scale_spline\n",
    "        self.enable_standalone_scale_spline = enable_standalone_scale_spline\n",
    "        self.base_activation = base_activation()\n",
    "        self.grid_eps = grid_eps\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.kaiming_uniform_(self.base_weight, a=math.sqrt(5) * self.scale_base)\n",
    "        with torch.no_grad():\n",
    "            noise = (\n",
    "                (\n",
    "                    torch.rand(self.grid_size + 1, self.in_features, self.out_features)\n",
    "                    - 1 / 2\n",
    "                )\n",
    "                * self.scale_noise\n",
    "                / self.grid_size\n",
    "            )\n",
    "            self.spline_weight.data.copy_(\n",
    "                (self.scale_spline if not self.enable_standalone_scale_spline else 1.0)\n",
    "                * self.curve2coeff(\n",
    "                    self.grid.T[self.spline_order : -self.spline_order],\n",
    "                    noise,\n",
    "                )\n",
    "            )\n",
    "            if self.enable_standalone_scale_spline:\n",
    "                # torch.nn.init.constant_(self.spline_scaler, self.scale_spline)\n",
    "                torch.nn.init.kaiming_uniform_(self.spline_scaler, a=math.sqrt(5) * self.scale_spline)\n",
    "\n",
    "    def b_splines(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute the B-spline bases for the given input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: B-spline bases tensor of shape (batch_size, in_features, grid_size + spline_order).\n",
    "        \"\"\"\n",
    "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
    "\n",
    "        grid: torch.Tensor = (\n",
    "            self.grid\n",
    "        )  # (in_features, grid_size + 2 * spline_order + 1)\n",
    "        x = x.unsqueeze(-1)\n",
    "        bases = ((x >= grid[:, :-1]) & (x < grid[:, 1:])).to(x.dtype)\n",
    "        for k in range(1, self.spline_order + 1):\n",
    "            bases = (\n",
    "                (x - grid[:, : -(k + 1)])\n",
    "                / (grid[:, k:-1] - grid[:, : -(k + 1)])\n",
    "                * bases[:, :, :-1]\n",
    "            ) + (\n",
    "                (grid[:, k + 1 :] - x)\n",
    "                / (grid[:, k + 1 :] - grid[:, 1:(-k)])\n",
    "                * bases[:, :, 1:]\n",
    "            )\n",
    "\n",
    "        assert bases.size() == (\n",
    "            x.size(0),\n",
    "            self.in_features,\n",
    "            self.grid_size + self.spline_order,\n",
    "        )\n",
    "        return bases.contiguous()\n",
    "\n",
    "    def curve2coeff(self, x: torch.Tensor, y: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute the coefficients of the curve that interpolates the given points.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n",
    "            y (torch.Tensor): Output tensor of shape (batch_size, in_features, out_features).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Coefficients tensor of shape (out_features, in_features, grid_size + spline_order).\n",
    "        \"\"\"\n",
    "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
    "        assert y.size() == (x.size(0), self.in_features, self.out_features)\n",
    "\n",
    "        A = self.b_splines(x).transpose(\n",
    "            0, 1\n",
    "        )  # (in_features, batch_size, grid_size + spline_order)\n",
    "        B = y.transpose(0, 1)  # (in_features, batch_size, out_features)\n",
    "        solution = torch.linalg.lstsq(\n",
    "            A, B\n",
    "        ).solution  # (in_features, grid_size + spline_order, out_features)\n",
    "        result = solution.permute(\n",
    "            2, 0, 1\n",
    "        )  # (out_features, in_features, grid_size + spline_order)\n",
    "\n",
    "        assert result.size() == (\n",
    "            self.out_features,\n",
    "            self.in_features,\n",
    "            self.grid_size + self.spline_order,\n",
    "        )\n",
    "        return result.contiguous()\n",
    "\n",
    "    @property\n",
    "    def scaled_spline_weight(self):\n",
    "        return self.spline_weight * (\n",
    "            self.spline_scaler.unsqueeze(-1)\n",
    "            if self.enable_standalone_scale_spline\n",
    "            else 1.0\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        assert x.size(-1) == self.in_features\n",
    "        original_shape = x.shape\n",
    "        x = x.reshape(-1, self.in_features)\n",
    "\n",
    "        base_output = F.linear(self.base_activation(x), self.base_weight)\n",
    "        spline_output = F.linear(\n",
    "            self.b_splines(x).view(x.size(0), -1),\n",
    "            self.scaled_spline_weight.view(self.out_features, -1),\n",
    "        )\n",
    "        output = base_output + spline_output\n",
    "        \n",
    "        output = output.reshape(*original_shape[:-1], self.out_features)\n",
    "        return output\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_grid(self, x: torch.Tensor, margin=0.01):\n",
    "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
    "        batch = x.size(0)\n",
    "\n",
    "        splines = self.b_splines(x)  # (batch, in, coeff)\n",
    "        splines = splines.permute(1, 0, 2)  # (in, batch, coeff)\n",
    "        orig_coeff = self.scaled_spline_weight  # (out, in, coeff)\n",
    "        orig_coeff = orig_coeff.permute(1, 2, 0)  # (in, coeff, out)\n",
    "        unreduced_spline_output = torch.bmm(splines, orig_coeff)  # (in, batch, out)\n",
    "        unreduced_spline_output = unreduced_spline_output.permute(\n",
    "            1, 0, 2\n",
    "        )  # (batch, in, out)\n",
    "\n",
    "        # sort each channel individually to collect data distribution\n",
    "        x_sorted = torch.sort(x, dim=0)[0]\n",
    "        grid_adaptive = x_sorted[\n",
    "            torch.linspace(\n",
    "                0, batch - 1, self.grid_size + 1, dtype=torch.int64, device=x.device\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        uniform_step = (x_sorted[-1] - x_sorted[0] + 2 * margin) / self.grid_size\n",
    "        grid_uniform = (\n",
    "            torch.arange(\n",
    "                self.grid_size + 1, dtype=torch.float32, device=x.device\n",
    "            ).unsqueeze(1)\n",
    "            * uniform_step\n",
    "            + x_sorted[0]\n",
    "            - margin\n",
    "        )\n",
    "\n",
    "        grid = self.grid_eps * grid_uniform + (1 - self.grid_eps) * grid_adaptive\n",
    "        grid = torch.concatenate(\n",
    "            [\n",
    "                grid[:1]\n",
    "                - uniform_step\n",
    "                * torch.arange(self.spline_order, 0, -1, device=x.device).unsqueeze(1),\n",
    "                grid,\n",
    "                grid[-1:]\n",
    "                + uniform_step\n",
    "                * torch.arange(1, self.spline_order + 1, device=x.device).unsqueeze(1),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        self.grid.copy_(grid.T)\n",
    "        self.spline_weight.data.copy_(self.curve2coeff(x, unreduced_spline_output))\n",
    "\n",
    "    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n",
    "        \"\"\"\n",
    "        Compute the regularization loss.\n",
    "\n",
    "        This is a dumb simulation of the original L1 regularization as stated in the\n",
    "        paper, since the original one requires computing absolutes and entropy from the\n",
    "        expanded (batch, in_features, out_features) intermediate tensor, which is hidden\n",
    "        behind the F.linear function if we want an memory efficient implementation.\n",
    "\n",
    "        The L1 regularization is now computed as mean absolute value of the spline\n",
    "        weights. The authors implementation also includes this term in addition to the\n",
    "        sample-based regularization.\n",
    "        \"\"\"\n",
    "        l1_fake = self.spline_weight.abs().mean(-1)\n",
    "        regularization_loss_activation = l1_fake.sum()\n",
    "        p = l1_fake / regularization_loss_activation\n",
    "        regularization_loss_entropy = -torch.sum(p * p.log())\n",
    "        return (\n",
    "            regularize_activation * regularization_loss_activation\n",
    "            + regularize_entropy * regularization_loss_entropy\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T14:09:02.126929Z",
     "iopub.status.busy": "2024-07-26T14:09:02.126521Z",
     "iopub.status.idle": "2024-07-26T14:09:02.138815Z",
     "shell.execute_reply": "2024-07-26T14:09:02.137837Z",
     "shell.execute_reply.started": "2024-07-26T14:09:02.126899Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MSA(torch.nn.Module): \n",
    "    \"\"\"\n",
    "        Initializes the Multi-Head Self-Attention (MSA) module with the given dimensions.\n",
    "\n",
    "        Args:\n",
    "            d (int): The total dimension of the input.\n",
    "            n_heads (int): The number of attention heads.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "    \"\"\"\n",
    "    def __init__(self, d, n_heads): \n",
    "        super(MSA, self).__init__()\n",
    "        self.d = d \n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        assert d % n_heads == 0 \n",
    "        d_head = int(d / n_heads)\n",
    "        \n",
    "        self.q_mappings = torch.nn.ModuleList([KANLinear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.k_mappings = torch.nn.ModuleList([KANLinear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.v_mappings = torch.nn.ModuleList([KANLinear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.d_head = d_head\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, sequence): \n",
    "        result = [] \n",
    "        for sequence in sequence: \n",
    "            seq_res = [] \n",
    "            for head in range(self.n_heads): \n",
    "                q_map = self.q_mappings[head]\n",
    "                k_map = self.k_mappings[head]\n",
    "                v_map = self.v_mappings[head]\n",
    "                \n",
    "                seq = sequence[:, head*self.d_head: (head+1)*self.d_head]\n",
    "                q, k, v = q_map(seq), k_map(seq), v_map(seq)\n",
    "                \n",
    "                attention = self.softmax(q @ k.T / (self.d_head ** 0.5))\n",
    "                seq_res.append(attention @ v)\n",
    "            result.append(torch.hstack(seq_res))\n",
    "        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T14:09:53.028499Z",
     "iopub.status.busy": "2024-07-26T14:09:53.028060Z",
     "iopub.status.idle": "2024-07-26T14:09:53.045721Z",
     "shell.execute_reply": "2024-07-26T14:09:53.044806Z",
     "shell.execute_reply.started": "2024-07-26T14:09:53.028463Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class KAN_ViT(torch.nn.Module): \n",
    "    \"\"\"\n",
    "        Initializes a Vision Transformer (ViT) module.\n",
    "\n",
    "        Args:\n",
    "            chw (list/tuple of 3 ints): The input image shape.\n",
    "            n_patches (int, optional): The number of patches to split the image into. Defaults to 10.\n",
    "            n_blocks (int, optional): The number of blocks in the transformer encoder. Defaults to 2.\n",
    "            d_hidden (int, optional): The number of hidden dimensions in the transformer encoder. Defaults to 8.\n",
    "            n_heads (int, optional): The number of attention heads in each block. Defaults to 2.\n",
    "            out_d (int, optional): The number of output dimensions. Defaults to 10.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "    \"\"\"    \n",
    "    def __init__(self, chw, n_patches=10, n_blocks=2, d_hidden=8, n_heads=2, out_d=10): \n",
    "        super(KAN_ViT, self).__init__()\n",
    "        \n",
    "        self.chw = chw\n",
    "        self.n_patches = n_patches\n",
    "        self.n_blocks = n_blocks\n",
    "        self.n_heads = n_heads\n",
    "        self.d_hidden = d_hidden\n",
    "        \n",
    "        assert chw[1] % n_patches == 0 \n",
    "        assert chw[2] % n_patches == 0\n",
    "        \n",
    "        self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n",
    "\n",
    "        # Linear mapping\n",
    "        self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n",
    "        self.linear_mapper = KANLinear(self.input_d, self.d_hidden)\n",
    "\n",
    "        # Classification token\n",
    "        self.v_class = torch.nn.Parameter(torch.rand(1, self.d_hidden))\n",
    "\n",
    "        # Positional embedding\n",
    "        self.register_buffer('pos_embeddings', self.positional_embeddings(n_patches ** 2 + 1, d_hidden),\n",
    "                             persistent=False)\n",
    "\n",
    "        # Encoder blocks\n",
    "        self.blocks = torch.nn.ModuleList([MSA(d_hidden, n_heads) for _ in range(n_blocks)])\n",
    "\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            KANLinear(self.d_hidden, out_d),\n",
    "            torch.nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "    def patchify(self, images, n_patches): \n",
    "        \"\"\"\n",
    "        The purpose of this function is to break down the main image into multiple sub-images and map them. \n",
    "\n",
    "        Args:\n",
    "            images (_type_): The image passeed into this function. \n",
    "            n_patches (_type_): The number of sub-images that will be created.\n",
    "        \"\"\"\n",
    "        \n",
    "        n, c, h, w = images.shape\n",
    "        assert h == w, \"Only for square images\"\n",
    "        \n",
    "        patches = torch.zeros(n, n_patches**2, h * w * c // n_patches ** 2) # The equation to calculate the patches\n",
    "        patch_size = h // n_patches\n",
    "        \n",
    "        for idx, image in enumerate(images):\n",
    "            for i in range(n_patches): \n",
    "                for j in range(n_patches): \n",
    "                    patch = image[:, i * patch_size: (i + 1) * patch_size, j * patch_size: (j + 1) * patch_size]\n",
    "                    patches[idx, i * n_patches + j] = patch.flatten()\n",
    "        return patches\n",
    "\n",
    "    def positional_embeddings(self, seq_length, d): \n",
    "        \"\"\"\n",
    "        the purpose of this function is to find high and low interaction of a word with surrounding words. \n",
    "        We can do so by the following equation below: \n",
    "        \n",
    "        Args: \n",
    "            seq_length (int): The length of the sequence/sentence\n",
    "            d (int): The dimension of the embedding\n",
    "        \"\"\"\n",
    "        \n",
    "        result = torch.ones(seq_length, d)\n",
    "        for i in range(seq_length): \n",
    "            for j in range(d): \n",
    "                result[i][j] = numpy.sin(i / 10000 ** (j / d)) if j % 2 == 0 else numpy.cos(i / 10000 ** (j/ d))\n",
    "        return result\n",
    "\n",
    "    def forward(self, images):\n",
    "        n, c, h, w = images.shape\n",
    "        patches = self.patchify(images, self.n_patches).to(self.pos_embeddings.device)\n",
    "\n",
    "        # running tokenization\n",
    "        tokens = self.linear_mapper(patches)\n",
    "        tokens = torch.cat((self.v_class.expand(n, 1, -1), tokens), dim=1)\n",
    "        out = tokens + self.pos_embeddings.repeat(n, 1, 1)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            out = block(out)\n",
    "\n",
    "        out = out[:, 0]\n",
    "        return self.mlp(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "mnist_model = KAN_ViT((1, 28, 28), n_patches=7, n_blocks=2, d_hidden=8, n_heads=2, out_d=10).to(device)\n",
    "optimizer = Adam(mnist_model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T14:20:34.475617Z",
     "iopub.status.busy": "2024-07-26T14:20:34.474695Z",
     "iopub.status.idle": "2024-07-26T14:20:34.489316Z",
     "shell.execute_reply": "2024-07-26T14:20:34.488278Z",
     "shell.execute_reply.started": "2024-07-26T14:20:34.475584Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import datetime\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_pred_proba):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "    y_true_bin = torch.nn.functional.one_hot(\n",
    "        torch.tensor(y_true), num_classes=10).numpy()\n",
    "    roc_auc = roc_auc_score(y_true_bin, y_pred_proba,\n",
    "                            average='weighted', multi_class='ovr')\n",
    "\n",
    "    return accuracy, balanced_accuracy, f1, roc_auc\n",
    "\n",
    "def save_metrics(filename, epoch, phase, loss, accuracy, balanced_accuracy, f1, roc_auc):\n",
    "    os.makedirs('logs', exist_ok=True)\n",
    "    with open(f'logs/{filename}', 'a') as f:\n",
    "        f.write(f\"Epoch: {epoch}, Phase: {phase}\\n\")\n",
    "        f.write(f\"  Loss: {loss:.4f}\\n\")\n",
    "        f.write(f\"  Accuracy: {accuracy:.4f}\\n\")\n",
    "        f.write(f\"  Balanced Accuracy: {balanced_accuracy:.4f}\\n\")\n",
    "        f.write(f\"  F1 Score: {f1:.4f}\\n\")\n",
    "        f.write(f\"  ROC AUC: {roc_auc:.4f}\\n\\n\")\n",
    "\n",
    "def main(train_loader, test_loader, epochs: int):\n",
    "    print(\"Using device: \", device,\n",
    "          f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Create a unique filename and TensorBoard writer for this run\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_filename = f\"effkan_{epochs}epochs_{timestamp}.txt\"\n",
    "    train_writer = SummaryWriter(log_dir=f\"runs/effkan_{epochs}epochs_experiment_{timestamp}/train\")\n",
    "    test_writer = SummaryWriter(log_dir=f\"runs/effkan_{epochs}epochs_experiment_{timestamp}/test\")\n",
    "\n",
    "    for epoch in trange(epochs, desc=\"train\"):\n",
    "        train_loss = 0.0\n",
    "        y_true_train, y_pred_train, y_pred_proba_train = [], [], []\n",
    "\n",
    "        mnist_model.train()\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1} in training\", leave=False):\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_hat = mnist_model(x)\n",
    "            loss = criterion(y_hat, y)\n",
    "\n",
    "            train_loss += loss.detach().cpu().item() / len(train_loader)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            y_true_train.extend(y.cpu().numpy())\n",
    "            y_pred_train.extend(torch.argmax(y_hat, dim=1).cpu().numpy())\n",
    "            y_pred_proba_train.extend(torch.nn.functional.softmax(\n",
    "                y_hat, dim=1).detach().cpu().numpy())\n",
    "\n",
    "        # Calculate training metrics\n",
    "        accuracy, balanced_accuracy, f1, roc_auc = calculate_metrics(\n",
    "            y_true_train, y_pred_train, y_pred_proba_train)\n",
    "\n",
    "        # Log training metrics to TensorBoard\n",
    "        train_writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "        train_writer.add_scalar('Accuracy/train', accuracy, epoch)\n",
    "        train_writer.add_scalar('Balanced Accuracy/train', balanced_accuracy, epoch)\n",
    "        train_writer.add_scalar('F1 Score/train', f1, epoch)\n",
    "        train_writer.add_scalar('ROC AUC/train', roc_auc, epoch)\n",
    "        \n",
    "        print(f\"Epoch: {epoch+1}, Phase: Train\")\n",
    "        print(f\"  Loss: {train_loss:.4f}\")\n",
    "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"  Balanced Accuracy: {balanced_accuracy:.4f}\")\n",
    "        print(f\"  F1 Score: {f1:.4f}\")\n",
    "        print(f\"  ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "        # Save metrics for the last epoch\n",
    "        if epoch == epochs - 1:\n",
    "            save_metrics(log_filename, epoch + 1, \"Train\",\n",
    "                         train_loss, accuracy, balanced_accuracy, f1, roc_auc)\n",
    "        train_writer.flush()\n",
    "\n",
    "    # Testing\n",
    "    mnist_model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_loss = 0.0\n",
    "        y_true_test, y_pred_test, y_pred_proba_test = [], [], []\n",
    "\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_hat = mnist_model(x)\n",
    "            loss = criterion(y_hat, y)\n",
    "            test_loss += loss.detach().cpu().item() / len(test_loader)\n",
    "\n",
    "            y_true_test.extend(y.cpu().numpy())\n",
    "            y_pred_test.extend(torch.argmax(y_hat, dim=1).cpu().numpy())\n",
    "            y_pred_proba_test.extend(\n",
    "                torch.nn.functional.softmax(y_hat, dim=1).cpu().numpy())\n",
    "\n",
    "        # Calculate test metrics\n",
    "        accuracy, balanced_accuracy, f1, roc_auc = calculate_metrics(\n",
    "            y_true_test, y_pred_test, y_pred_proba_test)\n",
    "\n",
    "        # Log test metrics to TensorBoard\n",
    "        test_writer.add_scalar('Loss/test', test_loss, epochs)\n",
    "        test_writer.add_scalar('Accuracy/test', accuracy, epochs)\n",
    "        test_writer.add_scalar('Balanced Accuracy/test', balanced_accuracy, epochs)\n",
    "        test_writer.add_scalar('F1 Score/test', f1, epochs)\n",
    "        test_writer.add_scalar('ROC AUC/test', roc_auc, epochs)\n",
    "        \n",
    "        print(f\"Epoch: {epochs}, Phase: Train\")\n",
    "        print(f\"  Loss: {test_loss:.4f}\")\n",
    "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"  Balanced Accuracy: {balanced_accuracy:.4f}\")\n",
    "        print(f\"  F1 Score: {f1:.4f}\")\n",
    "        print(f\"  ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "        # Save test metrics\n",
    "        save_metrics(log_filename, epochs, \"Test\", test_loss,\n",
    "                     accuracy, balanced_accuracy, f1, roc_auc)\n",
    "        \n",
    "        test_writer.flush()\n",
    "\n",
    "    # Close the TensorBoard writer\n",
    "    train_writer.close()\n",
    "    test_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-07-26T14:20:37.923905Z",
     "iopub.status.busy": "2024-07-26T14:20:37.923519Z",
     "iopub.status.idle": "2024-07-26T18:11:50.949086Z",
     "shell.execute_reply": "2024-07-26T18:11:50.947804Z",
     "shell.execute_reply.started": "2024-07-26T14:20:37.923876Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "train_mnist = MNIST(root='./mnist', train=True, download=True, transform=transform)\n",
    "test_mnist = MNIST(root='./mnist', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_mnist, shuffle=True, batch_size=128)\n",
    "test_loader = DataLoader(test_mnist, shuffle=False, batch_size=128)\n",
    "main(train_loader=train_loader, test_loader=test_loader, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
