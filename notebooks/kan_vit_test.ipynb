{"cells":[{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-07-26T14:07:44.484243Z","iopub.status.busy":"2024-07-26T14:07:44.483401Z","iopub.status.idle":"2024-07-26T14:07:44.488497Z","shell.execute_reply":"2024-07-26T14:07:44.487549Z","shell.execute_reply.started":"2024-07-26T14:07:44.484212Z"},"trusted":true},"outputs":[],"source":["# Original code has been adapted from the KAN Paper.\n","import math\n","import numpy\n","import torch\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-07-26T14:07:44.497406Z","iopub.status.busy":"2024-07-26T14:07:44.496733Z","iopub.status.idle":"2024-07-26T14:07:44.504401Z","shell.execute_reply":"2024-07-26T14:07:44.503397Z","shell.execute_reply.started":"2024-07-26T14:07:44.497379Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'cuda'"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["torch.manual_seed(42)\n","numpy.random.seed(42)\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","device"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-07-26T14:07:44.697002Z","iopub.status.busy":"2024-07-26T14:07:44.696666Z","iopub.status.idle":"2024-07-26T14:07:44.732135Z","shell.execute_reply":"2024-07-26T14:07:44.731301Z","shell.execute_reply.started":"2024-07-26T14:07:44.696979Z"},"trusted":true},"outputs":[],"source":["class KANLinear(torch.nn.Module):\n","    def __init__(\n","        self,\n","        in_features,\n","        out_features,\n","        grid_size=5,\n","        spline_order=3,\n","        scale_noise=0.1,\n","        scale_base=1.0,\n","        scale_spline=1.0,\n","        enable_standalone_scale_spline=True,\n","        base_activation=torch.nn.SiLU,\n","        grid_eps=0.02,\n","        grid_range=[-1, 1],\n","    ):\n","        super(KANLinear, self).__init__()\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.grid_size = grid_size\n","        self.spline_order = spline_order\n","\n","        h = (grid_range[1] - grid_range[0]) / grid_size\n","        grid = (\n","            (\n","                torch.arange(-spline_order, grid_size + spline_order + 1) * h\n","                + grid_range[0]\n","            )\n","            .expand(in_features, -1)\n","            .contiguous()\n","        )\n","        self.register_buffer(\"grid\", grid)\n","\n","        self.base_weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))\n","        self.spline_weight = torch.nn.Parameter(\n","            torch.Tensor(out_features, in_features, grid_size + spline_order)\n","        )\n","        if enable_standalone_scale_spline:\n","            self.spline_scaler = torch.nn.Parameter(\n","                torch.Tensor(out_features, in_features)\n","            )\n","\n","        self.scale_noise = scale_noise\n","        self.scale_base = scale_base\n","        self.scale_spline = scale_spline\n","        self.enable_standalone_scale_spline = enable_standalone_scale_spline\n","        self.base_activation = base_activation()\n","        self.grid_eps = grid_eps\n","\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        torch.nn.init.kaiming_uniform_(self.base_weight, a=math.sqrt(5) * self.scale_base)\n","        with torch.no_grad():\n","            noise = (\n","                (\n","                    torch.rand(self.grid_size + 1, self.in_features, self.out_features)\n","                    - 1 / 2\n","                )\n","                * self.scale_noise\n","                / self.grid_size\n","            )\n","            self.spline_weight.data.copy_(\n","                (self.scale_spline if not self.enable_standalone_scale_spline else 1.0)\n","                * self.curve2coeff(\n","                    self.grid.T[self.spline_order : -self.spline_order],\n","                    noise,\n","                )\n","            )\n","            if self.enable_standalone_scale_spline:\n","                # torch.nn.init.constant_(self.spline_scaler, self.scale_spline)\n","                torch.nn.init.kaiming_uniform_(self.spline_scaler, a=math.sqrt(5) * self.scale_spline)\n","\n","    def b_splines(self, x: torch.Tensor):\n","        \"\"\"\n","        Compute the B-spline bases for the given input tensor.\n","\n","        Args:\n","            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n","\n","        Returns:\n","            torch.Tensor: B-spline bases tensor of shape (batch_size, in_features, grid_size + spline_order).\n","        \"\"\"\n","        assert x.dim() == 2 and x.size(1) == self.in_features\n","\n","        grid: torch.Tensor = (\n","            self.grid\n","        )  # (in_features, grid_size + 2 * spline_order + 1)\n","        x = x.unsqueeze(-1)\n","        bases = ((x >= grid[:, :-1]) & (x < grid[:, 1:])).to(x.dtype)\n","        for k in range(1, self.spline_order + 1):\n","            bases = (\n","                (x - grid[:, : -(k + 1)])\n","                / (grid[:, k:-1] - grid[:, : -(k + 1)])\n","                * bases[:, :, :-1]\n","            ) + (\n","                (grid[:, k + 1 :] - x)\n","                / (grid[:, k + 1 :] - grid[:, 1:(-k)])\n","                * bases[:, :, 1:]\n","            )\n","\n","        assert bases.size() == (\n","            x.size(0),\n","            self.in_features,\n","            self.grid_size + self.spline_order,\n","        )\n","        return bases.contiguous()\n","\n","    def curve2coeff(self, x: torch.Tensor, y: torch.Tensor):\n","        \"\"\"\n","        Compute the coefficients of the curve that interpolates the given points.\n","\n","        Args:\n","            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n","            y (torch.Tensor): Output tensor of shape (batch_size, in_features, out_features).\n","\n","        Returns:\n","            torch.Tensor: Coefficients tensor of shape (out_features, in_features, grid_size + spline_order).\n","        \"\"\"\n","        assert x.dim() == 2 and x.size(1) == self.in_features\n","        assert y.size() == (x.size(0), self.in_features, self.out_features)\n","\n","        A = self.b_splines(x).transpose(\n","            0, 1\n","        )  # (in_features, batch_size, grid_size + spline_order)\n","        B = y.transpose(0, 1)  # (in_features, batch_size, out_features)\n","        solution = torch.linalg.lstsq(\n","            A, B\n","        ).solution  # (in_features, grid_size + spline_order, out_features)\n","        result = solution.permute(\n","            2, 0, 1\n","        )  # (out_features, in_features, grid_size + spline_order)\n","\n","        assert result.size() == (\n","            self.out_features,\n","            self.in_features,\n","            self.grid_size + self.spline_order,\n","        )\n","        return result.contiguous()\n","\n","    @property\n","    def scaled_spline_weight(self):\n","        return self.spline_weight * (\n","            self.spline_scaler.unsqueeze(-1)\n","            if self.enable_standalone_scale_spline\n","            else 1.0\n","        )\n","\n","    def forward(self, x: torch.Tensor):\n","        assert x.size(-1) == self.in_features\n","        original_shape = x.shape\n","        x = x.reshape(-1, self.in_features)\n","\n","        base_output = F.linear(self.base_activation(x), self.base_weight)\n","        spline_output = F.linear(\n","            self.b_splines(x).view(x.size(0), -1),\n","            self.scaled_spline_weight.view(self.out_features, -1),\n","        )\n","        output = base_output + spline_output\n","        \n","        output = output.reshape(*original_shape[:-1], self.out_features)\n","        return output\n","\n","    @torch.no_grad()\n","    def update_grid(self, x: torch.Tensor, margin=0.01):\n","        assert x.dim() == 2 and x.size(1) == self.in_features\n","        batch = x.size(0)\n","\n","        splines = self.b_splines(x)  # (batch, in, coeff)\n","        splines = splines.permute(1, 0, 2)  # (in, batch, coeff)\n","        orig_coeff = self.scaled_spline_weight  # (out, in, coeff)\n","        orig_coeff = orig_coeff.permute(1, 2, 0)  # (in, coeff, out)\n","        unreduced_spline_output = torch.bmm(splines, orig_coeff)  # (in, batch, out)\n","        unreduced_spline_output = unreduced_spline_output.permute(\n","            1, 0, 2\n","        )  # (batch, in, out)\n","\n","        # sort each channel individually to collect data distribution\n","        x_sorted = torch.sort(x, dim=0)[0]\n","        grid_adaptive = x_sorted[\n","            torch.linspace(\n","                0, batch - 1, self.grid_size + 1, dtype=torch.int64, device=x.device\n","            )\n","        ]\n","\n","        uniform_step = (x_sorted[-1] - x_sorted[0] + 2 * margin) / self.grid_size\n","        grid_uniform = (\n","            torch.arange(\n","                self.grid_size + 1, dtype=torch.float32, device=x.device\n","            ).unsqueeze(1)\n","            * uniform_step\n","            + x_sorted[0]\n","            - margin\n","        )\n","\n","        grid = self.grid_eps * grid_uniform + (1 - self.grid_eps) * grid_adaptive\n","        grid = torch.concatenate(\n","            [\n","                grid[:1]\n","                - uniform_step\n","                * torch.arange(self.spline_order, 0, -1, device=x.device).unsqueeze(1),\n","                grid,\n","                grid[-1:]\n","                + uniform_step\n","                * torch.arange(1, self.spline_order + 1, device=x.device).unsqueeze(1),\n","            ],\n","            dim=0,\n","        )\n","\n","        self.grid.copy_(grid.T)\n","        self.spline_weight.data.copy_(self.curve2coeff(x, unreduced_spline_output))\n","\n","    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n","        \"\"\"\n","        Compute the regularization loss.\n","\n","        This is a dumb simulation of the original L1 regularization as stated in the\n","        paper, since the original one requires computing absolutes and entropy from the\n","        expanded (batch, in_features, out_features) intermediate tensor, which is hidden\n","        behind the F.linear function if we want an memory efficient implementation.\n","\n","        The L1 regularization is now computed as mean absolute value of the spline\n","        weights. The authors implementation also includes this term in addition to the\n","        sample-based regularization.\n","        \"\"\"\n","        l1_fake = self.spline_weight.abs().mean(-1)\n","        regularization_loss_activation = l1_fake.sum()\n","        p = l1_fake / regularization_loss_activation\n","        regularization_loss_entropy = -torch.sum(p * p.log())\n","        return (\n","            regularize_activation * regularization_loss_activation\n","            + regularize_entropy * regularization_loss_entropy\n","        )\n"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-07-26T14:07:44.734399Z","iopub.status.busy":"2024-07-26T14:07:44.734073Z","iopub.status.idle":"2024-07-26T14:07:44.745230Z","shell.execute_reply":"2024-07-26T14:07:44.744527Z","shell.execute_reply.started":"2024-07-26T14:07:44.734370Z"},"trusted":true},"outputs":[],"source":["def patchify(images, n_patches): \n","    \"\"\"\n","    The purpose of this function is to break down the main image into multiple sub-images and map them. \n","\n","    Args:\n","        images (_type_): The image passeed into this function. \n","        n_patches (_type_): The number of sub-images that will be created.\n","    \"\"\"\n","    \n","    n, c, h, w = images.shape\n","    assert h == w, \"Only for square images\"\n","    \n","    patches = torch.zeros(n, n_patches**2, h * w * c // n_patches ** 2) # The equation to calculate the patches\n","    patch_size = h // n_patches\n","    \n","    for idx, image in enumerate(images):\n","        for i in range(n_patches): \n","            for j in range(n_patches): \n","                patch = image[:, i * patch_size: (i + 1) * patch_size, j * patch_size: (j + 1) * patch_size]\n","                patches[idx, i * n_patches + j] = patch.flatten()\n","    return patches"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-07-26T14:07:44.746620Z","iopub.status.busy":"2024-07-26T14:07:44.746362Z","iopub.status.idle":"2024-07-26T14:07:44.754067Z","shell.execute_reply":"2024-07-26T14:07:44.753282Z","shell.execute_reply.started":"2024-07-26T14:07:44.746598Z"},"trusted":true},"outputs":[],"source":["def positional_embeddings(seq_length, d): \n","    \"\"\"\n","    the purpose of this function is to find high and low interaction of a word with surrounding words. \n","    We can do so by the following equation below: \n","    \n","    Args: \n","        seq_length (int): The length of the sequence/sentence\n","        d (int): The dimension of the embedding\n","    \"\"\"\n","    \n","    result = torch.ones(seq_length, d)\n","    for i in range(seq_length): \n","        for j in range(d): \n","            result[i][j] = numpy.sin(i / 10000 ** (j / d)) if j % 2 == 0 else numpy.cos(i / 10000 ** (j/ d))\n","    return result"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-07-26T14:09:02.126929Z","iopub.status.busy":"2024-07-26T14:09:02.126521Z","iopub.status.idle":"2024-07-26T14:09:02.138815Z","shell.execute_reply":"2024-07-26T14:09:02.137837Z","shell.execute_reply.started":"2024-07-26T14:09:02.126899Z"},"trusted":true},"outputs":[],"source":["class MSA(torch.nn.Module): \n","    \"\"\"\n","        Initializes the Multi-Head Self-Attention (MSA) module with the given dimensions.\n","\n","        Args:\n","            d (int): The total dimension of the input.\n","            n_heads (int): The number of attention heads.\n","\n","        Returns:\n","            None\n","    \"\"\"\n","    def __init__(self, d, n_heads): \n","        super(MSA, self).__init__()\n","        self.d = d \n","        self.n_heads = n_heads\n","        \n","        assert d % n_heads == 0 \n","        d_head = int(d / n_heads)\n","        \n","        self.q_mappings = torch.nn.ModuleList([KANLinear(d_head, d_head) for _ in range(self.n_heads)])\n","        self.k_mappings = torch.nn.ModuleList([KANLinear(d_head, d_head) for _ in range(self.n_heads)])\n","        self.v_mappings = torch.nn.ModuleList([KANLinear(d_head, d_head) for _ in range(self.n_heads)])\n","        self.d_head = d_head\n","        self.softmax = torch.nn.Softmax(dim=-1)\n","        \n","    def forward(self, sequence): \n","        result = [] \n","        for sequence in sequence: \n","            seq_res = [] \n","            for head in range(self.n_heads): \n","                q_map = self.q_mappings[head]\n","                k_map = self.k_mappings[head]\n","                v_map = self.v_mappings[head]\n","                \n","                seq = sequence[:, head*self.d_head: (head+1)*self.d_head]\n","                q, k, v = q_map(seq), k_map(seq), v_map(seq)\n","                \n","                attention = self.softmax(q @ k.T / (self.d_head ** 0.5))\n","                seq_res.append(attention @ v)\n","            result.append(torch.hstack(seq_res))\n","        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-07-26T14:09:21.905111Z","iopub.status.busy":"2024-07-26T14:09:21.904707Z","iopub.status.idle":"2024-07-26T14:09:21.912780Z","shell.execute_reply":"2024-07-26T14:09:21.911812Z","shell.execute_reply.started":"2024-07-26T14:09:21.905082Z"},"trusted":true},"outputs":[],"source":["class Residual(torch.nn.Module): \n","    \"\"\"\n","        Initializes a Residual module.\n","\n","        Args:\n","            d_hidden (int): The number of hidden dimensions.\n","            n_heads (int): The number of attention heads.\n","            mlp_ratio (int, optional): The ratio of the number of hidden dimensions in the MLP layer. Defaults to 4.\n","\n","        Returns:\n","            None\n","    \"\"\"\n","    def __init___(self, d_hidden, n_heads, mlp_ratio=4): \n","        super(Residual, self).__init__()\n","        self.d_hidden = d_hidden\n","        self.n_heads = n_heads\n","        self.norm1 = torch.nn.LayerNorm(d_hidden)\n","        self.mhsa = MSA(d_hidden, n_heads)\n","        self.ml = torch.nn.Sequential(\n","            KANLinear(d_hidden, mlp_ratio * d_hidden), \n","            torch.nn.GELU(), \n","            KANLinear(mlp_ratio * d_hidden, d_hidden)\n","        )\n","        \n","    def forward(self, x): \n","        out = x = self.mhsa(self.norm1(x))\n","        out = out + self.mlp(self.norm2(x))\n","        return out"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-07-26T14:09:53.028499Z","iopub.status.busy":"2024-07-26T14:09:53.028060Z","iopub.status.idle":"2024-07-26T14:09:53.045721Z","shell.execute_reply":"2024-07-26T14:09:53.044806Z","shell.execute_reply.started":"2024-07-26T14:09:53.028463Z"},"trusted":true},"outputs":[],"source":["class KAN_ViT(torch.nn.Module): \n","    \"\"\"\n","        Initializes a Vision Transformer (ViT) module.\n","\n","        Args:\n","            chw (list/tuple of 3 ints): The input image shape.\n","            n_patches (int, optional): The number of patches to split the image into. Defaults to 10.\n","            n_blocks (int, optional): The number of blocks in the transformer encoder. Defaults to 2.\n","            d_hidden (int, optional): The number of hidden dimensions in the transformer encoder. Defaults to 8.\n","            n_heads (int, optional): The number of attention heads in each block. Defaults to 2.\n","            out_d (int, optional): The number of output dimensions. Defaults to 10.\n","\n","        Returns:\n","            None\n","    \"\"\"    \n","    def __init__(self, chw, n_patches=10, n_blocks=2, d_hidden=8, n_heads=2, out_d=10): \n","        super(KAN_ViT, self).__init__()\n","        \n","        self.chw = chw\n","        self.n_patches = n_patches\n","        self.n_blocks = n_blocks\n","        self.n_heads = n_heads\n","        self.d_hidden = d_hidden\n","        \n","        assert chw[1] % n_patches == 0 \n","        assert chw[2] % n_patches == 0\n","        \n","        self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n","\n","        # Linear mapping\n","        self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n","        self.linear_mapper = KANLinear(self.input_d, self.d_hidden)\n","\n","        # Classification token\n","        self.v_class = torch.nn.Parameter(torch.rand(1, self.d_hidden))\n","\n","        # Positional embedding\n","        self.register_buffer('positional_embeddings', positional_embeddings(n_patches ** 2 + 1, d_hidden),\n","                             persistent=False)\n","\n","        # Encoder blocks\n","        self.blocks = torch.nn.ModuleList([MSA(d_hidden, n_heads) for _ in range(n_blocks)])\n","\n","        self.mlp = torch.nn.Sequential(\n","            KANLinear(self.d_hidden, out_d),\n","            torch.nn.Softmax(dim=-1)\n","        )\n","\n","    def forward(self, images):\n","        n, c, h, w = images.shape\n","        patches = patchify(images, self.n_patches).to(self.positional_embeddings.device)\n","\n","        # running tokenization\n","        tokens = self.linear_mapper(patches)\n","        tokens = torch.cat((self.v_class.expand(n, 1, -1), tokens), dim=1)\n","        out = tokens + self.positional_embeddings.repeat(n, 1, 1)\n","\n","        for block in self.blocks:\n","            out = block(out)\n","\n","        out = out[:, 0]\n","        return self.mlp(out)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","mnist_model = KAN_ViT((1, 28, 28), n_patches=7, n_blocks=2, d_hidden=8, n_heads=2, out_d=10).to(device)\n","optimizer = Adam(mnist_model.parameters(), lr=0.005)"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-07-26T14:20:34.475617Z","iopub.status.busy":"2024-07-26T14:20:34.474695Z","iopub.status.idle":"2024-07-26T14:20:34.489316Z","shell.execute_reply":"2024-07-26T14:20:34.488278Z","shell.execute_reply.started":"2024-07-26T14:20:34.475584Z"},"trusted":true},"outputs":[],"source":["from torch.optim import Adam\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from torchvision.datasets import MNIST\n","from tqdm import tqdm, trange\n","\n","def main(train_loader, test_loader):\n","    \"\"\"\n","    This code contains the training and testing loop for training the vision transformers model. It requires two\n","    parameters\n","\n","    :param train_loader: The dataloader for the training set for training the model.\n","    :param test_loader: The dataloader for the testing set during evaluation phase.\n","    \"\"\"\n","    print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n","\n","    epochs = 5\n","    criterion = torch.nn.CrossEntropyLoss()\n","    for epoch in trange(epochs, desc=\"train\"):\n","        train_loss = 0.0\n","        for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1} in training\", leave=False):\n","            x, y = batch\n","            x, y = x.to(device), y.to(device)\n","            y_hat = mnist_model(x)\n","            loss = criterion(y_hat, y)\n","\n","            train_loss += loss.detach().cpu().item() / len(train_loader)\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","        print(f\"Epoch {epoch + 1}/{epochs} loss: {train_loss:.2f}\")\n","\n","    with torch.no_grad():\n","        correct, total = 0, 0\n","        test_loss = 0.0\n","        for batch in tqdm(test_loader, desc=\"Testing\"):\n","            x, y = batch\n","            x, y = x.to(device), y.to(device)\n","            y_hat = mnist_model(x)\n","            loss = criterion(y_hat, y)\n","            test_loss += loss.detach().cpu().item() / len(test_loader)\n","\n","            correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n","            total += len(x)\n","\n","        print(f\"Test loss: {test_loss:.2f}\")\n","        print(f\"Test accuracy: {correct / total * 100:.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-07-26T14:20:37.923905Z","iopub.status.busy":"2024-07-26T14:20:37.923519Z","iopub.status.idle":"2024-07-26T18:11:50.949086Z","shell.execute_reply":"2024-07-26T18:11:50.947804Z","shell.execute_reply.started":"2024-07-26T14:20:37.923876Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["transform = transforms.ToTensor()\n","train_mnist = MNIST(root='./mnist', train=True, download=True, transform=transform)\n","test_mnist = MNIST(root='./mnist', train=False, download=True, transform=transform)\n","train_loader = DataLoader(train_mnist, shuffle=True, batch_size=128)\n","test_loader = DataLoader(test_mnist, shuffle=False, batch_size=128)\n","main(train_loader=train_loader, test_loader=test_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Model's state_dict\")\n","for param_tensor in mnist_model.state_dict(): \n","  print(param_tensor, \"\\t\", mnist_model.state_dict()[param_tensor].size())\n","\n","print(\"Optim's state_dict\")\n","for var in optimizer.state_dict(): \n","  print(var, \"\\t\", optimizer.state_dict()[var])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["path: str = \"/kan_vit_5epochs.pth\"\n","\n","torch.save(mnist_model.state_dict(), path)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}
