{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebuTLORXNuxR",
        "outputId": "594f4b68-9a57-4849-d3c6-f44bf13d7fb2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7e5f783b44d0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.data import DataLoader\n",
        "# Uncomment this line for MNIST training.\n",
        "from torchvision.datasets.mnist import MNIST\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm, trange\n",
        "from torch.optim import Adam\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def patchify(images, n_patches):\n",
        "    \"\"\"\n",
        "    In order to \"sequentially\" pass in the images, we can break down the main image into multiple sub-images\n",
        "    and map them to a vector. This is exactly what this function does.\n",
        "\n",
        "    Arguments:\n",
        "    images: The image passed into this function\n",
        "    n_patches: The number of patches to split the image into.\n",
        "\n",
        "    Returns our patches aka the sub-images.\n",
        "    \"\"\"\n",
        "    n, c, h, w = images.shape\n",
        "\n",
        "    assert h == w, \"Only for square images\"\n",
        "\n",
        "    patches = torch.zeros(n, n_patches ** 2, h * w * c // n_patches ** 2)\n",
        "    patch_size = h // n_patches\n",
        "\n",
        "    for idx, image in enumerate(images):\n",
        "        for i in range(n_patches):\n",
        "            for j in range(n_patches):\n",
        "                patch = image[:, i * patch_size: (i + 1) * patch_size, j * patch_size: (j + 1) * patch_size]\n",
        "                patches[idx, i * n_patches + j] = patch.flatten()\n",
        "    return patches\n",
        "\n",
        "\n",
        "def positional_embeddings(sequence_length, d):\n",
        "    \"\"\"\n",
        "    In order for the model to know where to place each image, one can use positional embeddings where high freq values\n",
        "    are classified into the first few dimensions while low frequency values are added on to the latter dimensions. This\n",
        "    function performs exactly that. It has two parameters.\n",
        "\n",
        "    Arguments:\n",
        "    sequence_length: The number of tokens for the dataset.\n",
        "    d: The dimensionality for each token.\n",
        "\n",
        "    Returns a matrix where each (i,j) is added as token i in dimension j.\n",
        "    \"\"\"\n",
        "    result = torch.ones(sequence_length, d)\n",
        "    for i in range(sequence_length):\n",
        "        for j in range(d):\n",
        "            result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** (j / d)))\n",
        "    return result\n",
        "\n",
        "\n",
        "class MSA(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    This is the template implementation of the \"Multi-Scale Attention\" Layer.\n",
        "\n",
        "    The query, key and value mapping are matrix-multipled against each other in order to\n",
        "    find the attention, or, the relation of a word and its interaction with surrounding words.\n",
        "    \"\"\"\n",
        "    def __init__(self, d, n_heads=4):\n",
        "        super(MSA, self).__init__()\n",
        "        self.d = d\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        assert d % n_heads == 0  # Shouldn't divide dimension (d) into n_heads\n",
        "\n",
        "        d_head = int(d / n_heads)\n",
        "        self.q_mappings = torch.nn.ModuleList([torch.nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
        "        self.k_mappings = torch.nn.ModuleList([torch.nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
        "        self.v_mappings = torch.nn.ModuleList([torch.nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
        "        self.d_head = d_head\n",
        "        self.softmax = torch.nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, sequences):\n",
        "        result = []\n",
        "        for sequence in sequences:\n",
        "            seq_result = []\n",
        "            for head in range(self.n_heads):\n",
        "                q_mapping = self.q_mappings[head]\n",
        "                k_mapping = self.k_mappings[head]\n",
        "                v_mapping = self.v_mappings[head]\n",
        "\n",
        "                seq = sequence[:, head * self.d_head: (head + 1) * self.d_head]\n",
        "                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n",
        "\n",
        "                attention = self.softmax(q @ k.T / (self.d_head ** 0.5))\n",
        "                seq_result.append(attention @ v)\n",
        "            result.append(torch.hstack(seq_result))\n",
        "        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])\n",
        "\n",
        "\n",
        "class Residual(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    This is how a Residual Layer is built. The MSA that we have written will be a part\n",
        "    of this residual block right here.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_d, n_heads, mlp_ratio=4):\n",
        "        super(Residual, self).__init__()\n",
        "        self.hidden_d = hidden_d\n",
        "        self.n_heads = n_heads\n",
        "        self.norm1 = torch.nn.LayerNorm(hidden_d)\n",
        "        self.mhsa = MSA(hidden_d, n_heads)\n",
        "        self.mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(hidden_d, mlp_ratio * hidden_d),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Linear(mlp_ratio * hidden_d, hidden_d)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x + self.mhsa(self.norm1(x))\n",
        "        out = out + self.mlp(self.norm2(out))\n",
        "        return out\n",
        "\n",
        "\n",
        "class ViT(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    The workflow will be as follows.\n",
        "        1. Find the linear mapping of the input\n",
        "        2. Embed them using the function that we have written\n",
        "        3. Use 'n' MSA blocks and add a linear and a softmax layer at the end\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, chw, n_patches=16, n_blocks=2, hidden_d=8, n_heads=4, out_d=10):\n",
        "        super(ViT, self).__init__()\n",
        "\n",
        "        self.chw = chw\n",
        "        self.n_patches = n_patches\n",
        "        self.n_blocks = n_blocks\n",
        "        self.n_heads = n_heads\n",
        "        self.hidden_d = hidden_d\n",
        "\n",
        "        # Input and patch sizes\n",
        "        assert chw[1] % n_patches == 0\n",
        "        assert chw[2] % n_patches == 0\n",
        "        self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n",
        "\n",
        "        # Linear mapping\n",
        "        self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n",
        "        self.linear_mapper = torch.nn.Linear(self.input_d, self.hidden_d)\n",
        "\n",
        "        # Classification token\n",
        "        self.v_class = torch.nn.Parameter(torch.rand(1, self.hidden_d))\n",
        "\n",
        "        # Positional embedding\n",
        "        self.register_buffer('positional_embeddings', positional_embeddings(n_patches ** 2 + 1, hidden_d),\n",
        "                             persistent=False)\n",
        "\n",
        "        # Encoder blocks\n",
        "        self.blocks = torch.nn.ModuleList([MSA(hidden_d, n_heads) for _ in range(n_blocks)])\n",
        "\n",
        "        self.mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(self.hidden_d, out_d),\n",
        "            torch.nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "    def forward(self, images):\n",
        "        n, c, h, w = images.shape\n",
        "        patches = patchify(images, self.n_patches).to(self.positional_embeddings.device)\n",
        "\n",
        "        # rutorch.nning tokenization\n",
        "        tokens = self.linear_mapper(patches)\n",
        "        tokens = torch.cat((self.v_class.expand(n, 1, -1), tokens), dim=1)\n",
        "        out = tokens + self.positional_embeddings.repeat(n, 1, 1)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            out = block(out)\n",
        "\n",
        "        out = out[:, 0]\n",
        "        return self.mlp(out)"
      ],
      "metadata": {
        "id": "RPGqxBMNTcMU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(train_loader, test_loader):\n",
        "    \"\"\"\n",
        "    This code contains the training and testing loop for training the vision transformers model. It requires two\n",
        "    parameters\n",
        "\n",
        "    :param train_loader: The dataloader for the training set for training the model.\n",
        "    :param test_loader: The dataloader for the testing set during evaluation phase.\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    mnist_model = ViT((1, 28, 28), n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10).to(device)\n",
        "    print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n",
        "\n",
        "    epochs = 1\n",
        "    lr = 0.005\n",
        "\n",
        "    optimizer = Adam(mnist_model.parameters(), lr=lr)\n",
        "    criterion = CrossEntropyLoss()\n",
        "    for epoch in trange(epochs, desc=\"train\"):\n",
        "        train_loss = 0.0\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1} in training\", leave=False):\n",
        "            x, y = batch\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            y_hat = mnist_model(x)\n",
        "            loss = criterion(y_hat, y)\n",
        "\n",
        "            train_loss += loss.detach().cpu().item() / len(train_loader)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f\"Epoch {epoch + 1}/{epochs} loss: {train_loss:.2f}\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        correct, total = 0, 0\n",
        "        test_loss = 0.0\n",
        "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
        "            x, y = batch\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            y_hat = mnist_model(x)\n",
        "            loss = criterion(y_hat, y)\n",
        "            test_loss += loss.detach().cpu().item() / len(test_loader)\n",
        "\n",
        "            correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n",
        "            total += len(x)\n",
        "\n",
        "        print(f\"Test loss: {test_loss:.2f}\")\n",
        "        print(f\"Test accuracy: {correct / total * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "RUx7Ee94OR9f"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For MNIST: comment out the lines above and uncomment the lines below.\n",
        "\n",
        "transform = transforms.ToTensor()\n",
        "train_mnist = MNIST(root='./mnist', train=True, download=True, transform=transform)\n",
        "test_mnist = MNIST(root='./mnist', train=False, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_mnist, shuffle=True, batch_size=128)\n",
        "test_loader = DataLoader(test_mnist, shuffle=False, batch_size=128)\n",
        "main(train_loader=train_loader, test_loader=test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYa8_XejN69n",
        "outputId": "0901cf5d-588e-4ada-80e8-129a5cce6dc5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device:  cuda (Tesla T4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "Epoch 1 in training:   0%|          | 0/469 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1 in training:   0%|          | 1/469 [00:00<05:28,  1.43it/s]\u001b[A\n",
            "Epoch 1 in training:   0%|          | 2/469 [00:01<05:27,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:   1%|          | 3/469 [00:02<05:28,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:   1%|          | 4/469 [00:02<05:26,  1.43it/s]\u001b[A\n",
            "Epoch 1 in training:   1%|          | 5/469 [00:03<05:25,  1.43it/s]\u001b[A\n",
            "Epoch 1 in training:   1%|▏         | 6/469 [00:04<05:26,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:   1%|▏         | 7/469 [00:04<05:26,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:   2%|▏         | 8/469 [00:05<05:35,  1.38it/s]\u001b[A\n",
            "Epoch 1 in training:   2%|▏         | 9/469 [00:06<05:30,  1.39it/s]\u001b[A\n",
            "Epoch 1 in training:   2%|▏         | 10/469 [00:07<05:28,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:   2%|▏         | 11/469 [00:07<05:26,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:   3%|▎         | 12/469 [00:08<05:45,  1.32it/s]\u001b[A\n",
            "Epoch 1 in training:   3%|▎         | 13/469 [00:09<06:17,  1.21it/s]\u001b[A\n",
            "Epoch 1 in training:   3%|▎         | 14/469 [00:10<06:31,  1.16it/s]\u001b[A\n",
            "Epoch 1 in training:   3%|▎         | 15/469 [00:11<06:08,  1.23it/s]\u001b[A\n",
            "Epoch 1 in training:   3%|▎         | 16/469 [00:12<05:54,  1.28it/s]\u001b[A\n",
            "Epoch 1 in training:   4%|▎         | 17/469 [00:12<05:40,  1.33it/s]\u001b[A\n",
            "Epoch 1 in training:   4%|▍         | 18/469 [00:13<05:32,  1.36it/s]\u001b[A\n",
            "Epoch 1 in training:   4%|▍         | 19/469 [00:14<05:29,  1.36it/s]\u001b[A\n",
            "Epoch 1 in training:   4%|▍         | 20/469 [00:14<05:23,  1.39it/s]\u001b[A\n",
            "Epoch 1 in training:   4%|▍         | 21/469 [00:15<05:18,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:   5%|▍         | 22/469 [00:16<05:17,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:   5%|▍         | 23/469 [00:16<05:15,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:   5%|▌         | 24/469 [00:17<05:12,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:   5%|▌         | 25/469 [00:18<05:10,  1.43it/s]\u001b[A\n",
            "Epoch 1 in training:   6%|▌         | 26/469 [00:18<05:09,  1.43it/s]\u001b[A\n",
            "Epoch 1 in training:   6%|▌         | 27/469 [00:19<05:06,  1.44it/s]\u001b[A\n",
            "Epoch 1 in training:   6%|▌         | 28/469 [00:20<05:10,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:   6%|▌         | 29/469 [00:21<05:43,  1.28it/s]\u001b[A\n",
            "Epoch 1 in training:   6%|▋         | 30/469 [00:22<06:24,  1.14it/s]\u001b[A\n",
            "Epoch 1 in training:   7%|▋         | 31/469 [00:23<06:03,  1.21it/s]\u001b[A\n",
            "Epoch 1 in training:   7%|▋         | 32/469 [00:23<05:43,  1.27it/s]\u001b[A\n",
            "Epoch 1 in training:   7%|▋         | 33/469 [00:24<05:30,  1.32it/s]\u001b[A\n",
            "Epoch 1 in training:   7%|▋         | 34/469 [00:25<05:21,  1.35it/s]\u001b[A\n",
            "Epoch 1 in training:   7%|▋         | 35/469 [00:25<05:14,  1.38it/s]\u001b[A\n",
            "Epoch 1 in training:   8%|▊         | 36/469 [00:26<05:08,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:   8%|▊         | 37/469 [00:27<05:06,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:   8%|▊         | 38/469 [00:28<05:03,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:   8%|▊         | 39/469 [00:28<05:01,  1.43it/s]\u001b[A\n",
            "Epoch 1 in training:   9%|▊         | 40/469 [00:29<05:04,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:   9%|▊         | 41/469 [00:30<05:04,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:   9%|▉         | 42/469 [00:30<05:01,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:   9%|▉         | 43/469 [00:31<05:00,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:   9%|▉         | 44/469 [00:32<04:57,  1.43it/s]\u001b[A\n",
            "Epoch 1 in training:  10%|▉         | 45/469 [00:33<05:15,  1.35it/s]\u001b[A\n",
            "Epoch 1 in training:  10%|▉         | 46/469 [00:34<05:43,  1.23it/s]\u001b[A\n",
            "Epoch 1 in training:  10%|█         | 47/469 [00:34<05:57,  1.18it/s]\u001b[A\n",
            "Epoch 1 in training:  10%|█         | 48/469 [00:35<05:36,  1.25it/s]\u001b[A\n",
            "Epoch 1 in training:  10%|█         | 49/469 [00:36<05:22,  1.30it/s]\u001b[A\n",
            "Epoch 1 in training:  11%|█         | 50/469 [00:37<05:13,  1.33it/s]\u001b[A\n",
            "Epoch 1 in training:  11%|█         | 51/469 [00:37<05:08,  1.36it/s]\u001b[A\n",
            "Epoch 1 in training:  11%|█         | 52/469 [00:38<05:02,  1.38it/s]\u001b[A\n",
            "Epoch 1 in training:  11%|█▏        | 53/469 [00:39<04:56,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:  12%|█▏        | 54/469 [00:39<04:59,  1.39it/s]\u001b[A\n",
            "Epoch 1 in training:  12%|█▏        | 55/469 [00:40<04:57,  1.39it/s]\u001b[A\n",
            "Epoch 1 in training:  12%|█▏        | 56/469 [00:41<04:53,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  12%|█▏        | 57/469 [00:42<04:51,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  12%|█▏        | 58/469 [00:42<04:53,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:  13%|█▎        | 59/469 [00:43<05:07,  1.33it/s]\u001b[A\n",
            "Epoch 1 in training:  13%|█▎        | 60/469 [00:44<04:58,  1.37it/s]\u001b[A\n",
            "Epoch 1 in training:  13%|█▎        | 61/469 [00:45<05:08,  1.32it/s]\u001b[A\n",
            "Epoch 1 in training:  13%|█▎        | 62/469 [00:46<05:36,  1.21it/s]\u001b[A\n",
            "Epoch 1 in training:  13%|█▎        | 63/469 [00:47<05:57,  1.14it/s]\u001b[A\n",
            "Epoch 1 in training:  14%|█▎        | 64/469 [00:47<05:34,  1.21it/s]\u001b[A\n",
            "Epoch 1 in training:  14%|█▍        | 65/469 [00:48<05:16,  1.28it/s]\u001b[A\n",
            "Epoch 1 in training:  14%|█▍        | 66/469 [00:49<05:05,  1.32it/s]\u001b[A\n",
            "Epoch 1 in training:  14%|█▍        | 67/469 [00:49<05:01,  1.34it/s]\u001b[A\n",
            "Epoch 1 in training:  14%|█▍        | 68/469 [00:50<04:53,  1.36it/s]\u001b[A\n",
            "Epoch 1 in training:  15%|█▍        | 69/469 [00:51<04:48,  1.38it/s]\u001b[A\n",
            "Epoch 1 in training:  15%|█▍        | 70/469 [00:51<04:45,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:  15%|█▌        | 71/469 [00:52<04:42,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  15%|█▌        | 72/469 [00:53<04:39,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  16%|█▌        | 73/469 [00:54<04:37,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  16%|█▌        | 74/469 [00:54<04:36,  1.43it/s]\u001b[A\n",
            "Epoch 1 in training:  16%|█▌        | 75/469 [00:55<04:35,  1.43it/s]\u001b[A\n",
            "Epoch 1 in training:  16%|█▌        | 76/469 [00:56<04:35,  1.43it/s]\u001b[A\n",
            "Epoch 1 in training:  16%|█▋        | 77/469 [00:56<04:33,  1.43it/s]\u001b[A\n",
            "Epoch 1 in training:  17%|█▋        | 78/469 [00:57<05:00,  1.30it/s]\u001b[A\n",
            "Epoch 1 in training:  17%|█▋        | 79/469 [00:58<05:30,  1.18it/s]\u001b[A\n",
            "Epoch 1 in training:  17%|█▋        | 80/469 [00:59<05:28,  1.18it/s]\u001b[A\n",
            "Epoch 1 in training:  17%|█▋        | 81/469 [01:00<05:12,  1.24it/s]\u001b[A\n",
            "Epoch 1 in training:  17%|█▋        | 82/469 [01:01<04:58,  1.30it/s]\u001b[A\n",
            "Epoch 1 in training:  18%|█▊        | 83/469 [01:01<04:50,  1.33it/s]\u001b[A\n",
            "Epoch 1 in training:  18%|█▊        | 84/469 [01:02<04:43,  1.36it/s]\u001b[A\n",
            "Epoch 1 in training:  18%|█▊        | 85/469 [01:03<04:38,  1.38it/s]\u001b[A\n",
            "Epoch 1 in training:  18%|█▊        | 86/469 [01:03<04:34,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:  19%|█▊        | 87/469 [01:04<04:31,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  19%|█▉        | 88/469 [01:05<04:32,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:  19%|█▉        | 89/469 [01:05<04:30,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  19%|█▉        | 90/469 [01:06<04:29,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  19%|█▉        | 91/469 [01:07<04:30,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:  20%|█▉        | 92/469 [01:08<04:27,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  20%|█▉        | 93/469 [01:08<04:25,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  20%|██        | 94/469 [01:09<04:43,  1.32it/s]\u001b[A\n",
            "Epoch 1 in training:  20%|██        | 95/469 [01:10<05:12,  1.20it/s]\u001b[A\n",
            "Epoch 1 in training:  20%|██        | 96/469 [01:11<05:26,  1.14it/s]\u001b[A\n",
            "Epoch 1 in training:  21%|██        | 97/469 [01:12<05:05,  1.22it/s]\u001b[A\n",
            "Epoch 1 in training:  21%|██        | 98/469 [01:13<04:50,  1.28it/s]\u001b[A\n",
            "Epoch 1 in training:  21%|██        | 99/469 [01:13<04:41,  1.31it/s]\u001b[A\n",
            "Epoch 1 in training:  21%|██▏       | 100/469 [01:14<04:32,  1.35it/s]\u001b[A\n",
            "Epoch 1 in training:  22%|██▏       | 101/469 [01:15<04:28,  1.37it/s]\u001b[A\n",
            "Epoch 1 in training:  22%|██▏       | 102/469 [01:15<04:24,  1.39it/s]\u001b[A\n",
            "Epoch 1 in training:  22%|██▏       | 103/469 [01:16<04:21,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:  22%|██▏       | 104/469 [01:17<04:16,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  22%|██▏       | 105/469 [01:17<04:16,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  23%|██▎       | 106/469 [01:18<04:16,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  23%|██▎       | 107/469 [01:19<04:14,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  23%|██▎       | 108/469 [01:20<04:12,  1.43it/s]\u001b[A\n",
            "Epoch 1 in training:  23%|██▎       | 109/469 [01:20<04:11,  1.43it/s]\u001b[A\n",
            "Epoch 1 in training:  23%|██▎       | 110/469 [01:21<04:16,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:  24%|██▎       | 111/469 [01:22<04:41,  1.27it/s]\u001b[A\n",
            "Epoch 1 in training:  24%|██▍       | 112/469 [01:23<05:09,  1.15it/s]\u001b[A\n",
            "Epoch 1 in training:  24%|██▍       | 113/469 [01:24<04:56,  1.20it/s]\u001b[A\n",
            "Epoch 1 in training:  24%|██▍       | 114/469 [01:24<04:41,  1.26it/s]\u001b[A\n",
            "Epoch 1 in training:  25%|██▍       | 115/469 [01:25<04:29,  1.31it/s]\u001b[A\n",
            "Epoch 1 in training:  25%|██▍       | 116/469 [01:26<04:24,  1.33it/s]\u001b[A\n",
            "Epoch 1 in training:  25%|██▍       | 117/469 [01:27<04:18,  1.36it/s]\u001b[A\n",
            "Epoch 1 in training:  25%|██▌       | 118/469 [01:27<04:12,  1.39it/s]\u001b[A\n",
            "Epoch 1 in training:  25%|██▌       | 119/469 [01:28<04:09,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:  26%|██▌       | 120/469 [01:29<04:08,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  26%|██▌       | 121/469 [01:29<04:05,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  26%|██▌       | 122/469 [01:30<04:03,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  26%|██▌       | 123/469 [01:31<04:05,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  26%|██▋       | 124/469 [01:31<04:04,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  27%|██▋       | 125/469 [01:32<04:04,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  27%|██▋       | 126/469 [01:33<04:02,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  27%|██▋       | 127/469 [01:34<04:21,  1.31it/s]\u001b[A\n",
            "Epoch 1 in training:  27%|██▋       | 128/469 [01:35<04:46,  1.19it/s]\u001b[A\n",
            "Epoch 1 in training:  28%|██▊       | 129/469 [01:36<04:50,  1.17it/s]\u001b[A\n",
            "Epoch 1 in training:  28%|██▊       | 130/469 [01:36<04:32,  1.24it/s]\u001b[A\n",
            "Epoch 1 in training:  28%|██▊       | 131/469 [01:37<04:21,  1.29it/s]\u001b[A\n",
            "Epoch 1 in training:  28%|██▊       | 132/469 [01:38<04:13,  1.33it/s]\u001b[A\n",
            "Epoch 1 in training:  28%|██▊       | 133/469 [01:39<04:07,  1.36it/s]\u001b[A\n",
            "Epoch 1 in training:  29%|██▊       | 134/469 [01:39<04:03,  1.38it/s]\u001b[A\n",
            "Epoch 1 in training:  29%|██▉       | 135/469 [01:40<04:01,  1.38it/s]\u001b[A\n",
            "Epoch 1 in training:  29%|██▉       | 136/469 [01:41<03:57,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:  29%|██▉       | 137/469 [01:41<03:56,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  29%|██▉       | 138/469 [01:42<03:55,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:  30%|██▉       | 139/469 [01:43<03:54,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:  30%|██▉       | 140/469 [01:43<03:53,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  30%|███       | 141/469 [01:44<03:53,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  30%|███       | 142/469 [01:45<03:50,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  30%|███       | 143/469 [01:46<04:00,  1.35it/s]\u001b[A\n",
            "Epoch 1 in training:  31%|███       | 144/469 [01:47<04:22,  1.24it/s]\u001b[A\n",
            "Epoch 1 in training:  31%|███       | 145/469 [01:48<04:40,  1.15it/s]\u001b[A\n",
            "Epoch 1 in training:  31%|███       | 146/469 [01:48<04:23,  1.22it/s]\u001b[A\n",
            "Epoch 1 in training:  31%|███▏      | 147/469 [01:49<04:13,  1.27it/s]\u001b[A\n",
            "Epoch 1 in training:  32%|███▏      | 148/469 [01:50<04:02,  1.32it/s]\u001b[A\n",
            "Epoch 1 in training:  32%|███▏      | 149/469 [01:50<03:56,  1.36it/s]\u001b[A\n",
            "Epoch 1 in training:  32%|███▏      | 150/469 [01:51<03:52,  1.37it/s]\u001b[A\n",
            "Epoch 1 in training:  32%|███▏      | 151/469 [01:52<03:47,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:  32%|███▏      | 152/469 [01:53<03:48,  1.39it/s]\u001b[A\n",
            "Epoch 1 in training:  33%|███▎      | 153/469 [01:53<03:46,  1.39it/s]\u001b[A\n",
            "Epoch 1 in training:  33%|███▎      | 154/469 [01:54<03:42,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  33%|███▎      | 155/469 [01:55<03:40,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  33%|███▎      | 156/469 [01:55<03:40,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  33%|███▎      | 157/469 [01:56<03:38,  1.43it/s]\u001b[A\n",
            "Epoch 1 in training:  34%|███▎      | 158/469 [01:57<03:36,  1.44it/s]\u001b[A\n",
            "Epoch 1 in training:  34%|███▍      | 159/469 [01:57<03:36,  1.43it/s]\u001b[A\n",
            "Epoch 1 in training:  34%|███▍      | 160/469 [01:58<03:57,  1.30it/s]\u001b[A\n",
            "Epoch 1 in training:  34%|███▍      | 161/469 [01:59<04:19,  1.19it/s]\u001b[A\n",
            "Epoch 1 in training:  35%|███▍      | 162/469 [02:00<04:14,  1.21it/s]\u001b[A\n",
            "Epoch 1 in training:  35%|███▍      | 163/469 [02:01<04:01,  1.27it/s]\u001b[A\n",
            "Epoch 1 in training:  35%|███▍      | 164/469 [02:02<03:51,  1.32it/s]\u001b[A\n",
            "Epoch 1 in training:  35%|███▌      | 165/469 [02:02<03:45,  1.35it/s]\u001b[A\n",
            "Epoch 1 in training:  35%|███▌      | 166/469 [02:03<03:41,  1.37it/s]\u001b[A\n",
            "Epoch 1 in training:  36%|███▌      | 167/469 [02:04<03:39,  1.38it/s]\u001b[A\n",
            "Epoch 1 in training:  36%|███▌      | 168/469 [02:04<03:35,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:  36%|███▌      | 169/469 [02:05<03:37,  1.38it/s]\u001b[A\n",
            "Epoch 1 in training:  36%|███▌      | 170/469 [02:06<03:33,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:  36%|███▋      | 171/469 [02:07<03:33,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:  37%|███▋      | 172/469 [02:07<03:29,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  37%|███▋      | 173/469 [02:08<03:29,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  37%|███▋      | 174/469 [02:09<03:27,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  37%|███▋      | 175/469 [02:09<03:25,  1.43it/s]\u001b[A\n",
            "Epoch 1 in training:  38%|███▊      | 176/469 [02:10<03:41,  1.32it/s]\u001b[A\n",
            "Epoch 1 in training:  38%|███▊      | 177/469 [02:11<04:01,  1.21it/s]\u001b[A\n",
            "Epoch 1 in training:  38%|███▊      | 178/469 [02:12<04:07,  1.17it/s]\u001b[A\n",
            "Epoch 1 in training:  38%|███▊      | 179/469 [02:13<03:54,  1.24it/s]\u001b[A\n",
            "Epoch 1 in training:  38%|███▊      | 180/469 [02:14<03:44,  1.29it/s]\u001b[A\n",
            "Epoch 1 in training:  39%|███▊      | 181/469 [02:14<03:38,  1.32it/s]\u001b[A\n",
            "Epoch 1 in training:  39%|███▉      | 182/469 [02:15<03:32,  1.35it/s]\u001b[A\n",
            "Epoch 1 in training:  39%|███▉      | 183/469 [02:16<03:27,  1.38it/s]\u001b[A\n",
            "Epoch 1 in training:  39%|███▉      | 184/469 [02:16<03:24,  1.39it/s]\u001b[A\n",
            "Epoch 1 in training:  39%|███▉      | 185/469 [02:17<03:22,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:  40%|███▉      | 186/469 [02:18<03:19,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  40%|███▉      | 187/469 [02:18<03:18,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  40%|████      | 188/469 [02:19<03:16,  1.43it/s]\u001b[A\n",
            "Epoch 1 in training:  40%|████      | 189/469 [02:20<03:13,  1.44it/s]\u001b[A\n",
            "Epoch 1 in training:  41%|████      | 190/469 [02:20<03:14,  1.44it/s]\u001b[A\n",
            "Epoch 1 in training:  41%|████      | 191/469 [02:21<03:13,  1.44it/s]\u001b[A\n",
            "Epoch 1 in training:  41%|████      | 192/469 [02:22<03:15,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  41%|████      | 193/469 [02:23<03:31,  1.31it/s]\u001b[A\n",
            "Epoch 1 in training:  41%|████▏     | 194/469 [02:24<03:55,  1.17it/s]\u001b[A\n",
            "Epoch 1 in training:  42%|████▏     | 195/469 [02:25<03:45,  1.21it/s]\u001b[A\n",
            "Epoch 1 in training:  42%|████▏     | 196/469 [02:25<03:35,  1.27it/s]\u001b[A\n",
            "Epoch 1 in training:  42%|████▏     | 197/469 [02:26<03:29,  1.30it/s]\u001b[A\n",
            "Epoch 1 in training:  42%|████▏     | 198/469 [02:27<03:22,  1.34it/s]\u001b[A\n",
            "Epoch 1 in training:  42%|████▏     | 199/469 [02:27<03:18,  1.36it/s]\u001b[A\n",
            "Epoch 1 in training:  43%|████▎     | 200/469 [02:28<03:14,  1.38it/s]\u001b[A\n",
            "Epoch 1 in training:  43%|████▎     | 201/469 [02:29<03:11,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:  43%|████▎     | 202/469 [02:30<03:09,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  43%|████▎     | 203/469 [02:30<03:07,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  43%|████▎     | 204/469 [02:31<03:04,  1.43it/s]\u001b[A\n",
            "Epoch 1 in training:  44%|████▎     | 205/469 [02:32<03:05,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  44%|████▍     | 206/469 [02:32<03:05,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  44%|████▍     | 207/469 [02:33<03:01,  1.44it/s]\u001b[A\n",
            "Epoch 1 in training:  44%|████▍     | 208/469 [02:34<03:03,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  45%|████▍     | 209/469 [02:35<03:16,  1.33it/s]\u001b[A\n",
            "Epoch 1 in training:  45%|████▍     | 210/469 [02:36<03:35,  1.20it/s]\u001b[A\n",
            "Epoch 1 in training:  45%|████▍     | 211/469 [02:37<03:41,  1.16it/s]\u001b[A\n",
            "Epoch 1 in training:  45%|████▌     | 212/469 [02:37<03:28,  1.23it/s]\u001b[A\n",
            "Epoch 1 in training:  45%|████▌     | 213/469 [02:38<03:18,  1.29it/s]\u001b[A\n",
            "Epoch 1 in training:  46%|████▌     | 214/469 [02:39<03:13,  1.32it/s]\u001b[A\n",
            "Epoch 1 in training:  46%|████▌     | 215/469 [02:39<03:07,  1.35it/s]\u001b[A\n",
            "Epoch 1 in training:  46%|████▌     | 216/469 [02:40<03:05,  1.36it/s]\u001b[A\n",
            "Epoch 1 in training:  46%|████▋     | 217/469 [02:41<03:01,  1.39it/s]\u001b[A\n",
            "Epoch 1 in training:  46%|████▋     | 218/469 [02:42<03:08,  1.33it/s]\u001b[A\n",
            "Epoch 1 in training:  47%|████▋     | 219/469 [02:42<03:04,  1.36it/s]\u001b[A\n",
            "Epoch 1 in training:  47%|████▋     | 220/469 [02:43<03:01,  1.37it/s]\u001b[A\n",
            "Epoch 1 in training:  47%|████▋     | 221/469 [02:44<03:00,  1.37it/s]\u001b[A\n",
            "Epoch 1 in training:  47%|████▋     | 222/469 [02:44<02:57,  1.39it/s]\u001b[A\n",
            "Epoch 1 in training:  48%|████▊     | 223/469 [02:45<02:55,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:  48%|████▊     | 224/469 [02:46<02:54,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:  48%|████▊     | 225/469 [02:47<03:04,  1.32it/s]\u001b[A\n",
            "Epoch 1 in training:  48%|████▊     | 226/469 [02:48<03:19,  1.22it/s]\u001b[A\n",
            "Epoch 1 in training:  48%|████▊     | 227/469 [02:49<03:29,  1.15it/s]\u001b[A\n",
            "Epoch 1 in training:  49%|████▊     | 228/469 [02:49<03:16,  1.22it/s]\u001b[A\n",
            "Epoch 1 in training:  49%|████▉     | 229/469 [02:50<03:06,  1.28it/s]\u001b[A\n",
            "Epoch 1 in training:  49%|████▉     | 230/469 [02:51<03:00,  1.33it/s]\u001b[A\n",
            "Epoch 1 in training:  49%|████▉     | 231/469 [02:51<02:56,  1.35it/s]\u001b[A\n",
            "Epoch 1 in training:  49%|████▉     | 232/469 [02:52<02:52,  1.38it/s]\u001b[A\n",
            "Epoch 1 in training:  50%|████▉     | 233/469 [02:53<02:49,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:  50%|████▉     | 234/469 [02:54<02:46,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  50%|█████     | 235/469 [02:54<02:44,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  50%|█████     | 236/469 [02:55<02:44,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  51%|█████     | 237/469 [02:56<02:44,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  51%|█████     | 238/469 [02:56<02:45,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:  51%|█████     | 239/469 [02:57<02:44,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:  51%|█████     | 240/469 [02:58<02:43,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:  51%|█████▏    | 241/469 [02:59<02:46,  1.37it/s]\u001b[A\n",
            "Epoch 1 in training:  52%|█████▏    | 242/469 [02:59<02:58,  1.27it/s]\u001b[A\n",
            "Epoch 1 in training:  52%|█████▏    | 243/469 [03:01<03:18,  1.14it/s]\u001b[A\n",
            "Epoch 1 in training:  52%|█████▏    | 244/469 [03:01<03:05,  1.21it/s]\u001b[A\n",
            "Epoch 1 in training:  52%|█████▏    | 245/469 [03:02<02:57,  1.27it/s]\u001b[A\n",
            "Epoch 1 in training:  52%|█████▏    | 246/469 [03:03<02:50,  1.31it/s]\u001b[A\n",
            "Epoch 1 in training:  53%|█████▎    | 247/469 [03:03<02:45,  1.34it/s]\u001b[A\n",
            "Epoch 1 in training:  53%|█████▎    | 248/469 [03:04<02:42,  1.36it/s]\u001b[A\n",
            "Epoch 1 in training:  53%|█████▎    | 249/469 [03:05<02:38,  1.39it/s]\u001b[A\n",
            "Epoch 1 in training:  53%|█████▎    | 250/469 [03:06<02:39,  1.37it/s]\u001b[A\n",
            "Epoch 1 in training:  54%|█████▎    | 251/469 [03:06<02:40,  1.35it/s]\u001b[A\n",
            "Epoch 1 in training:  54%|█████▎    | 252/469 [03:07<02:38,  1.37it/s]\u001b[A\n",
            "Epoch 1 in training:  54%|█████▍    | 253/469 [03:08<02:36,  1.38it/s]\u001b[A\n",
            "Epoch 1 in training:  54%|█████▍    | 254/469 [03:08<02:34,  1.39it/s]\u001b[A\n",
            "Epoch 1 in training:  54%|█████▍    | 255/469 [03:09<02:33,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:  55%|█████▍    | 256/469 [03:10<02:33,  1.39it/s]\u001b[A\n",
            "Epoch 1 in training:  55%|█████▍    | 257/469 [03:11<02:34,  1.38it/s]\u001b[A\n",
            "Epoch 1 in training:  55%|█████▌    | 258/469 [03:12<02:49,  1.24it/s]\u001b[A\n",
            "Epoch 1 in training:  55%|█████▌    | 259/469 [03:13<03:05,  1.13it/s]\u001b[A\n",
            "Epoch 1 in training:  55%|█████▌    | 260/469 [03:13<02:56,  1.19it/s]\u001b[A\n",
            "Epoch 1 in training:  56%|█████▌    | 261/469 [03:14<02:46,  1.25it/s]\u001b[A\n",
            "Epoch 1 in training:  56%|█████▌    | 262/469 [03:15<02:39,  1.30it/s]\u001b[A\n",
            "Epoch 1 in training:  56%|█████▌    | 263/469 [03:15<02:34,  1.33it/s]\u001b[A\n",
            "Epoch 1 in training:  56%|█████▋    | 264/469 [03:16<02:29,  1.37it/s]\u001b[A\n",
            "Epoch 1 in training:  57%|█████▋    | 265/469 [03:17<02:26,  1.39it/s]\u001b[A\n",
            "Epoch 1 in training:  57%|█████▋    | 266/469 [03:18<02:26,  1.39it/s]\u001b[A\n",
            "Epoch 1 in training:  57%|█████▋    | 267/469 [03:18<02:23,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  57%|█████▋    | 268/469 [03:19<02:21,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  57%|█████▋    | 269/469 [03:20<02:20,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  58%|█████▊    | 270/469 [03:20<02:19,  1.43it/s]\u001b[A\n",
            "Epoch 1 in training:  58%|█████▊    | 271/469 [03:21<02:19,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  58%|█████▊    | 272/469 [03:22<02:18,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  58%|█████▊    | 273/469 [03:22<02:17,  1.43it/s]\u001b[A\n",
            "Epoch 1 in training:  58%|█████▊    | 274/469 [03:23<02:26,  1.33it/s]\u001b[A\n",
            "Epoch 1 in training:  59%|█████▊    | 275/469 [03:24<02:41,  1.20it/s]\u001b[A\n",
            "Epoch 1 in training:  59%|█████▉    | 276/469 [03:25<02:46,  1.16it/s]\u001b[A\n",
            "Epoch 1 in training:  59%|█████▉    | 277/469 [03:26<02:35,  1.23it/s]\u001b[A\n",
            "Epoch 1 in training:  59%|█████▉    | 278/469 [03:27<02:28,  1.29it/s]\u001b[A\n",
            "Epoch 1 in training:  59%|█████▉    | 279/469 [03:27<02:23,  1.33it/s]\u001b[A\n",
            "Epoch 1 in training:  60%|█████▉    | 280/469 [03:28<02:20,  1.35it/s]\u001b[A\n",
            "Epoch 1 in training:  60%|█████▉    | 281/469 [03:29<02:16,  1.37it/s]\u001b[A\n",
            "Epoch 1 in training:  60%|██████    | 282/469 [03:30<02:14,  1.39it/s]\u001b[A\n",
            "Epoch 1 in training:  60%|██████    | 283/469 [03:30<02:13,  1.39it/s]\u001b[A\n",
            "Epoch 1 in training:  61%|██████    | 284/469 [03:31<02:12,  1.39it/s]\u001b[A\n",
            "Epoch 1 in training:  61%|██████    | 285/469 [03:32<02:10,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  61%|██████    | 286/469 [03:32<02:09,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  61%|██████    | 287/469 [03:33<02:08,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  61%|██████▏   | 288/469 [03:34<02:06,  1.43it/s]\u001b[A\n",
            "Epoch 1 in training:  62%|██████▏   | 289/469 [03:34<02:05,  1.43it/s]\u001b[A\n",
            "Epoch 1 in training:  62%|██████▏   | 290/469 [03:35<02:08,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:  62%|██████▏   | 291/469 [03:36<02:18,  1.29it/s]\u001b[A\n",
            "Epoch 1 in training:  62%|██████▏   | 292/469 [03:37<02:32,  1.16it/s]\u001b[A\n",
            "Epoch 1 in training:  62%|██████▏   | 293/469 [03:38<02:23,  1.22it/s]\u001b[A\n",
            "Epoch 1 in training:  63%|██████▎   | 294/469 [03:39<02:17,  1.27it/s]\u001b[A\n",
            "Epoch 1 in training:  63%|██████▎   | 295/469 [03:39<02:12,  1.32it/s]\u001b[A\n",
            "Epoch 1 in training:  63%|██████▎   | 296/469 [03:40<02:08,  1.35it/s]\u001b[A\n",
            "Epoch 1 in training:  63%|██████▎   | 297/469 [03:41<02:06,  1.36it/s]\u001b[A\n",
            "Epoch 1 in training:  64%|██████▎   | 298/469 [03:41<02:04,  1.38it/s]\u001b[A\n",
            "Epoch 1 in training:  64%|██████▍   | 299/469 [03:42<02:02,  1.39it/s]\u001b[A\n",
            "Epoch 1 in training:  64%|██████▍   | 300/469 [03:43<02:00,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:  64%|██████▍   | 301/469 [03:43<01:59,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  64%|██████▍   | 302/469 [03:44<01:58,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  65%|██████▍   | 303/469 [03:45<01:57,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  65%|██████▍   | 304/469 [03:46<01:56,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  65%|██████▌   | 305/469 [03:46<01:55,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  65%|██████▌   | 306/469 [03:47<01:54,  1.43it/s]\u001b[A\n",
            "Epoch 1 in training:  65%|██████▌   | 307/469 [03:48<02:02,  1.32it/s]\u001b[A\n",
            "Epoch 1 in training:  66%|██████▌   | 308/469 [03:49<02:14,  1.20it/s]\u001b[A\n",
            "Epoch 1 in training:  66%|██████▌   | 309/469 [03:50<02:15,  1.18it/s]\u001b[A\n",
            "Epoch 1 in training:  66%|██████▌   | 310/469 [03:50<02:07,  1.24it/s]\u001b[A\n",
            "Epoch 1 in training:  66%|██████▋   | 311/469 [03:51<02:01,  1.30it/s]\u001b[A\n",
            "Epoch 1 in training:  67%|██████▋   | 312/469 [03:52<01:57,  1.34it/s]\u001b[A\n",
            "Epoch 1 in training:  67%|██████▋   | 313/469 [03:53<01:54,  1.36it/s]\u001b[A\n",
            "Epoch 1 in training:  67%|██████▋   | 314/469 [03:53<01:51,  1.39it/s]\u001b[A\n",
            "Epoch 1 in training:  67%|██████▋   | 315/469 [03:54<01:49,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:  67%|██████▋   | 316/469 [03:55<01:48,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  68%|██████▊   | 317/469 [03:55<01:47,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  68%|██████▊   | 318/469 [03:56<01:46,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  68%|██████▊   | 319/469 [03:57<01:45,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  68%|██████▊   | 320/469 [03:57<01:44,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  68%|██████▊   | 321/469 [03:58<01:43,  1.43it/s]\u001b[A\n",
            "Epoch 1 in training:  69%|██████▊   | 322/469 [03:59<01:42,  1.43it/s]\u001b[A\n",
            "Epoch 1 in training:  69%|██████▉   | 323/469 [04:00<01:44,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:  69%|██████▉   | 324/469 [04:01<01:54,  1.27it/s]\u001b[A\n",
            "Epoch 1 in training:  69%|██████▉   | 325/469 [04:02<02:07,  1.13it/s]\u001b[A\n",
            "Epoch 1 in training:  70%|██████▉   | 326/469 [04:02<01:57,  1.22it/s]\u001b[A\n",
            "Epoch 1 in training:  70%|██████▉   | 327/469 [04:03<01:51,  1.27it/s]\u001b[A\n",
            "Epoch 1 in training:  70%|██████▉   | 328/469 [04:04<01:46,  1.32it/s]\u001b[A\n",
            "Epoch 1 in training:  70%|███████   | 329/469 [04:04<01:42,  1.36it/s]\u001b[A\n",
            "Epoch 1 in training:  70%|███████   | 330/469 [04:05<01:40,  1.38it/s]\u001b[A\n",
            "Epoch 1 in training:  71%|███████   | 331/469 [04:06<01:39,  1.39it/s]\u001b[A\n",
            "Epoch 1 in training:  71%|███████   | 332/469 [04:07<01:37,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  71%|███████   | 333/469 [04:07<01:36,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  71%|███████   | 334/469 [04:08<01:34,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  71%|███████▏  | 335/469 [04:09<01:33,  1.44it/s]\u001b[A\n",
            "Epoch 1 in training:  72%|███████▏  | 336/469 [04:09<01:33,  1.43it/s]\u001b[A\n",
            "Epoch 1 in training:  72%|███████▏  | 337/469 [04:10<01:32,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  72%|███████▏  | 338/469 [04:11<01:32,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  72%|███████▏  | 339/469 [04:11<01:31,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  72%|███████▏  | 340/469 [04:12<01:38,  1.32it/s]\u001b[A\n",
            "Epoch 1 in training:  73%|███████▎  | 341/469 [04:13<01:46,  1.20it/s]\u001b[A\n",
            "Epoch 1 in training:  73%|███████▎  | 342/469 [04:14<01:47,  1.18it/s]\u001b[A\n",
            "Epoch 1 in training:  73%|███████▎  | 343/469 [04:15<01:40,  1.25it/s]\u001b[A\n",
            "Epoch 1 in training:  73%|███████▎  | 344/469 [04:16<01:35,  1.31it/s]\u001b[A\n",
            "Epoch 1 in training:  74%|███████▎  | 345/469 [04:16<01:32,  1.34it/s]\u001b[A\n",
            "Epoch 1 in training:  74%|███████▍  | 346/469 [04:17<01:29,  1.38it/s]\u001b[A\n",
            "Epoch 1 in training:  74%|███████▍  | 347/469 [04:18<01:27,  1.39it/s]\u001b[A\n",
            "Epoch 1 in training:  74%|███████▍  | 348/469 [04:18<01:26,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:  74%|███████▍  | 349/469 [04:19<01:25,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:  75%|███████▍  | 350/469 [04:20<01:23,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  75%|███████▍  | 351/469 [04:20<01:22,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  75%|███████▌  | 352/469 [04:21<01:22,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  75%|███████▌  | 353/469 [04:22<01:21,  1.43it/s]\u001b[A\n",
            "Epoch 1 in training:  75%|███████▌  | 354/469 [04:23<01:20,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  76%|███████▌  | 355/469 [04:23<01:20,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  76%|███████▌  | 356/469 [04:24<01:21,  1.38it/s]\u001b[A\n",
            "Epoch 1 in training:  76%|███████▌  | 357/469 [04:25<01:28,  1.26it/s]\u001b[A\n",
            "Epoch 1 in training:  76%|███████▋  | 358/469 [04:26<01:38,  1.13it/s]\u001b[A\n",
            "Epoch 1 in training:  77%|███████▋  | 359/469 [04:27<01:30,  1.21it/s]\u001b[A\n",
            "Epoch 1 in training:  77%|███████▋  | 360/469 [04:27<01:26,  1.27it/s]\u001b[A\n",
            "Epoch 1 in training:  77%|███████▋  | 361/469 [04:28<01:22,  1.32it/s]\u001b[A\n",
            "Epoch 1 in training:  77%|███████▋  | 362/469 [04:29<01:19,  1.35it/s]\u001b[A\n",
            "Epoch 1 in training:  77%|███████▋  | 363/469 [04:30<01:17,  1.36it/s]\u001b[A\n",
            "Epoch 1 in training:  78%|███████▊  | 364/469 [04:30<01:16,  1.38it/s]\u001b[A\n",
            "Epoch 1 in training:  78%|███████▊  | 365/469 [04:31<01:16,  1.37it/s]\u001b[A\n",
            "Epoch 1 in training:  78%|███████▊  | 366/469 [04:32<01:14,  1.38it/s]\u001b[A\n",
            "Epoch 1 in training:  78%|███████▊  | 367/469 [04:32<01:12,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:  78%|███████▊  | 368/469 [04:33<01:11,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  79%|███████▊  | 369/469 [04:34<01:10,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  79%|███████▉  | 370/469 [04:35<01:09,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  79%|███████▉  | 371/469 [04:35<01:08,  1.43it/s]\u001b[A\n",
            "Epoch 1 in training:  79%|███████▉  | 372/469 [04:36<01:08,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  80%|███████▉  | 373/469 [04:37<01:15,  1.27it/s]\u001b[A\n",
            "Epoch 1 in training:  80%|███████▉  | 374/469 [04:38<01:27,  1.09it/s]\u001b[A\n",
            "Epoch 1 in training:  80%|███████▉  | 375/469 [04:39<01:22,  1.14it/s]\u001b[A\n",
            "Epoch 1 in training:  80%|████████  | 376/469 [04:40<01:16,  1.21it/s]\u001b[A\n",
            "Epoch 1 in training:  80%|████████  | 377/469 [04:40<01:14,  1.23it/s]\u001b[A\n",
            "Epoch 1 in training:  81%|████████  | 378/469 [04:41<01:11,  1.28it/s]\u001b[A\n",
            "Epoch 1 in training:  81%|████████  | 379/469 [04:42<01:08,  1.32it/s]\u001b[A\n",
            "Epoch 1 in training:  81%|████████  | 380/469 [04:43<01:05,  1.35it/s]\u001b[A\n",
            "Epoch 1 in training:  81%|████████  | 381/469 [04:43<01:04,  1.37it/s]\u001b[A\n",
            "Epoch 1 in training:  81%|████████▏ | 382/469 [04:44<01:02,  1.38it/s]\u001b[A\n",
            "Epoch 1 in training:  82%|████████▏ | 383/469 [04:45<01:01,  1.39it/s]\u001b[A\n",
            "Epoch 1 in training:  82%|████████▏ | 384/469 [04:45<01:00,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:  82%|████████▏ | 385/469 [04:46<00:59,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  82%|████████▏ | 386/469 [04:47<00:58,  1.43it/s]\u001b[A\n",
            "Epoch 1 in training:  83%|████████▎ | 387/469 [04:47<00:57,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  83%|████████▎ | 388/469 [04:48<00:57,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  83%|████████▎ | 389/469 [04:49<01:01,  1.30it/s]\u001b[A\n",
            "Epoch 1 in training:  83%|████████▎ | 390/469 [04:50<01:06,  1.18it/s]\u001b[A\n",
            "Epoch 1 in training:  83%|████████▎ | 391/469 [04:51<01:06,  1.18it/s]\u001b[A\n",
            "Epoch 1 in training:  84%|████████▎ | 392/469 [04:52<01:01,  1.24it/s]\u001b[A\n",
            "Epoch 1 in training:  84%|████████▍ | 393/469 [04:52<00:58,  1.30it/s]\u001b[A\n",
            "Epoch 1 in training:  84%|████████▍ | 394/469 [04:53<00:56,  1.33it/s]\u001b[A\n",
            "Epoch 1 in training:  84%|████████▍ | 395/469 [04:54<00:54,  1.37it/s]\u001b[A\n",
            "Epoch 1 in training:  84%|████████▍ | 396/469 [04:54<00:52,  1.38it/s]\u001b[A\n",
            "Epoch 1 in training:  85%|████████▍ | 397/469 [04:55<00:52,  1.38it/s]\u001b[A\n",
            "Epoch 1 in training:  85%|████████▍ | 398/469 [04:56<00:50,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:  85%|████████▌ | 399/469 [04:57<00:49,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  85%|████████▌ | 400/469 [04:57<00:49,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  86%|████████▌ | 401/469 [04:58<00:47,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  86%|████████▌ | 402/469 [04:59<00:47,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  86%|████████▌ | 403/469 [04:59<00:46,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  86%|████████▌ | 404/469 [05:00<00:45,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  86%|████████▋ | 405/469 [05:01<00:47,  1.36it/s]\u001b[A\n",
            "Epoch 1 in training:  87%|████████▋ | 406/469 [05:02<00:50,  1.24it/s]\u001b[A\n",
            "Epoch 1 in training:  87%|████████▋ | 407/469 [05:03<00:53,  1.16it/s]\u001b[A\n",
            "Epoch 1 in training:  87%|████████▋ | 408/469 [05:04<00:49,  1.22it/s]\u001b[A\n",
            "Epoch 1 in training:  87%|████████▋ | 409/469 [05:04<00:46,  1.28it/s]\u001b[A\n",
            "Epoch 1 in training:  87%|████████▋ | 410/469 [05:05<00:44,  1.33it/s]\u001b[A\n",
            "Epoch 1 in training:  88%|████████▊ | 411/469 [05:06<00:43,  1.34it/s]\u001b[A\n",
            "Epoch 1 in training:  88%|████████▊ | 412/469 [05:06<00:41,  1.37it/s]\u001b[A\n",
            "Epoch 1 in training:  88%|████████▊ | 413/469 [05:07<00:40,  1.38it/s]\u001b[A\n",
            "Epoch 1 in training:  88%|████████▊ | 414/469 [05:08<00:39,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:  88%|████████▊ | 415/469 [05:08<00:38,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  89%|████████▊ | 416/469 [05:09<00:37,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  89%|████████▉ | 417/469 [05:10<00:36,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  89%|████████▉ | 418/469 [05:11<00:36,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:  89%|████████▉ | 419/469 [05:11<00:35,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  90%|████████▉ | 420/469 [05:12<00:34,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  90%|████████▉ | 421/469 [05:13<00:34,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:  90%|████████▉ | 422/469 [05:14<00:36,  1.28it/s]\u001b[A\n",
            "Epoch 1 in training:  90%|█████████ | 423/469 [05:15<00:40,  1.14it/s]\u001b[A\n",
            "Epoch 1 in training:  90%|█████████ | 424/469 [05:15<00:37,  1.20it/s]\u001b[A\n",
            "Epoch 1 in training:  91%|█████████ | 425/469 [05:16<00:34,  1.26it/s]\u001b[A\n",
            "Epoch 1 in training:  91%|█████████ | 426/469 [05:17<00:33,  1.30it/s]\u001b[A\n",
            "Epoch 1 in training:  91%|█████████ | 427/469 [05:18<00:31,  1.33it/s]\u001b[A\n",
            "Epoch 1 in training:  91%|█████████▏| 428/469 [05:18<00:30,  1.36it/s]\u001b[A\n",
            "Epoch 1 in training:  91%|█████████▏| 429/469 [05:19<00:28,  1.38it/s]\u001b[A\n",
            "Epoch 1 in training:  92%|█████████▏| 430/469 [05:20<00:27,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  92%|█████████▏| 431/469 [05:20<00:26,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  92%|█████████▏| 432/469 [05:21<00:26,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  92%|█████████▏| 433/469 [05:22<00:25,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  93%|█████████▎| 434/469 [05:22<00:24,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  93%|█████████▎| 435/469 [05:23<00:23,  1.43it/s]\u001b[A\n",
            "Epoch 1 in training:  93%|█████████▎| 436/469 [05:24<00:22,  1.44it/s]\u001b[A\n",
            "Epoch 1 in training:  93%|█████████▎| 437/469 [05:25<00:22,  1.43it/s]\u001b[A\n",
            "Epoch 1 in training:  93%|█████████▎| 438/469 [05:25<00:23,  1.33it/s]\u001b[A\n",
            "Epoch 1 in training:  94%|█████████▎| 439/469 [05:26<00:24,  1.22it/s]\u001b[A\n",
            "Epoch 1 in training:  94%|█████████▍| 440/469 [05:27<00:24,  1.17it/s]\u001b[A\n",
            "Epoch 1 in training:  94%|█████████▍| 441/469 [05:28<00:22,  1.24it/s]\u001b[A\n",
            "Epoch 1 in training:  94%|█████████▍| 442/469 [05:29<00:20,  1.29it/s]\u001b[A\n",
            "Epoch 1 in training:  94%|█████████▍| 443/469 [05:29<00:19,  1.33it/s]\u001b[A\n",
            "Epoch 1 in training:  95%|█████████▍| 444/469 [05:30<00:18,  1.36it/s]\u001b[A\n",
            "Epoch 1 in training:  95%|█████████▍| 445/469 [05:31<00:17,  1.39it/s]\u001b[A\n",
            "Epoch 1 in training:  95%|█████████▌| 446/469 [05:32<00:16,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:  95%|█████████▌| 447/469 [05:32<00:15,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:  96%|█████████▌| 448/469 [05:33<00:14,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  96%|█████████▌| 449/469 [05:34<00:14,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  96%|█████████▌| 450/469 [05:34<00:13,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  96%|█████████▌| 451/469 [05:35<00:12,  1.43it/s]\u001b[A\n",
            "Epoch 1 in training:  96%|█████████▋| 452/469 [05:36<00:12,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training:  97%|█████████▋| 453/469 [05:36<00:11,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training:  97%|█████████▋| 454/469 [05:37<00:10,  1.38it/s]\u001b[A\n",
            "Epoch 1 in training:  97%|█████████▋| 455/469 [05:38<00:11,  1.26it/s]\u001b[A\n",
            "Epoch 1 in training:  97%|█████████▋| 456/469 [05:39<00:11,  1.12it/s]\u001b[A\n",
            "Epoch 1 in training:  97%|█████████▋| 457/469 [05:40<00:09,  1.20it/s]\u001b[A\n",
            "Epoch 1 in training:  98%|█████████▊| 458/469 [05:41<00:08,  1.25it/s]\u001b[A\n",
            "Epoch 1 in training:  98%|█████████▊| 459/469 [05:41<00:07,  1.30it/s]\u001b[A\n",
            "Epoch 1 in training:  98%|█████████▊| 460/469 [05:42<00:06,  1.33it/s]\u001b[A\n",
            "Epoch 1 in training:  98%|█████████▊| 461/469 [05:43<00:05,  1.35it/s]\u001b[A\n",
            "Epoch 1 in training:  99%|█████████▊| 462/469 [05:44<00:05,  1.37it/s]\u001b[A\n",
            "Epoch 1 in training:  99%|█████████▊| 463/469 [05:44<00:04,  1.39it/s]\u001b[A\n",
            "Epoch 1 in training:  99%|█████████▉| 464/469 [05:45<00:03,  1.38it/s]\u001b[A\n",
            "Epoch 1 in training:  99%|█████████▉| 465/469 [05:46<00:02,  1.40it/s]\u001b[A\n",
            "Epoch 1 in training:  99%|█████████▉| 466/469 [05:46<00:02,  1.41it/s]\u001b[A\n",
            "Epoch 1 in training: 100%|█████████▉| 467/469 [05:47<00:01,  1.42it/s]\u001b[A\n",
            "Epoch 1 in training: 100%|█████████▉| 468/469 [05:48<00:00,  1.43it/s]\u001b[A\n",
            "Epoch 1 in training: 100%|██████████| 469/469 [05:48<00:00,  1.53it/s]\u001b[A\n",
            "train: 100%|██████████| 1/1 [05:48<00:00, 348.82s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1 loss: 2.15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 79/79 [00:33<00:00,  2.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 2.08\n",
            "Test accuracy: 37.75%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}