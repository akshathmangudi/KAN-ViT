{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-08-07T10:35:23.860444Z","iopub.status.busy":"2024-08-07T10:35:23.860063Z","iopub.status.idle":"2024-08-07T10:35:25.496663Z","shell.execute_reply":"2024-08-07T10:35:25.495696Z","shell.execute_reply.started":"2024-08-07T10:35:23.860413Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math\n","from typing import *\n","from torch.autograd import Function"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from pathlib import Path\n","import numpy as np\n","from torch.nn import CrossEntropyLoss\n","from torch.utils.data import DataLoader\n","# Uncomment this line for MNIST training.\n","from torchvision.datasets.mnist import MNIST\n","from torchvision import transforms\n","from tqdm import tqdm, trange\n","from torch.optim import Adam\n","\n","np.random.seed(42)\n","torch.manual_seed(42)"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T10:36:38.636464Z","iopub.status.busy":"2024-08-07T10:36:38.635530Z","iopub.status.idle":"2024-08-07T10:36:38.645191Z","shell.execute_reply":"2024-08-07T10:36:38.644300Z","shell.execute_reply.started":"2024-08-07T10:36:38.636431Z"},"trusted":true},"outputs":[],"source":["\n","class RSWAFFunction(Function):\n","    @staticmethod\n","    def forward(ctx, input, grid, inv_denominator, train_grid, train_inv_denominator):\n","        diff = (input[..., None] - grid)\n","        diff_mul = diff.mul(inv_denominator)\n","        tanh_diff = torch.tanh(diff)\n","        tanh_diff_deriviative = -tanh_diff.mul(tanh_diff) + 1  # sech^2(x) = 1 - tanh^2(x)\n","        \n","        # Save tensors for backward pass\n","        ctx.save_for_backward(input, tanh_diff, tanh_diff_deriviative, diff, inv_denominator)\n","        ctx.train_grid = train_grid\n","        ctx.train_inv_denominator = train_inv_denominator\n","        \n","        return tanh_diff_deriviative\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        # Retrieve saved tensors\n","        input, tanh_diff, tanh_diff_deriviative, diff, inv_denominator = ctx.saved_tensors\n","        grad_grid = None\n","        grad_inv_denominator = None\n","        grad_input = -2 * tanh_diff * tanh_diff_deriviative * grad_output\n","        grad_input = grad_input.sum(dim=-1).mul(inv_denominator)\n","        if ctx.train_grid:\n","            #print('\\n')\n","            #print(f\"grad_grid shape: {grad_grid.shape }\")\n","            grad_grid = -inv_denominator * grad_output.sum(dim=0).sum(dim=0)    \n","        if ctx.train_inv_denominator:\n","            grad_inv_denominator = (grad_output* diff).sum()\n","        return grad_input, grad_grid, grad_inv_denominator, None, None # same number as tensors or parameters"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T10:36:40.782295Z","iopub.status.busy":"2024-08-07T10:36:40.781926Z","iopub.status.idle":"2024-08-07T10:36:40.790303Z","shell.execute_reply":"2024-08-07T10:36:40.789325Z","shell.execute_reply.started":"2024-08-07T10:36:40.782245Z"},"trusted":true},"outputs":[],"source":["class ReflectionalSwitchFunction(nn.Module):\n","    def __init__(\n","        self,\n","        grid_min: float = -1.2,\n","        grid_max: float = 0.2,\n","        num_grids: int = 8,\n","        exponent: int = 2,\n","        inv_denominator: float = 0.5,\n","        train_grid: bool = False,        \n","        train_inv_denominator: bool = False,\n","    ):\n","        super().__init__()\n","        grid = torch.linspace(grid_min, grid_max, num_grids)\n","        self.train_grid = torch.tensor(train_grid, dtype=torch.bool)\n","        self.train_inv_denominator = torch.tensor(train_inv_denominator, dtype=torch.bool) \n","        self.grid = torch.nn.Parameter(grid, requires_grad=train_grid)\n","        #print(f\"grid initial shape: {self.grid.shape }\")\n","        self.inv_denominator = torch.nn.Parameter(torch.tensor(inv_denominator, dtype=torch.float32), requires_grad=train_inv_denominator)  # Cache the inverse of the denominator\n","\n","    def forward(self, x):\n","        return RSWAFFunction.apply(x, self.grid, self.inv_denominator, self.train_grid, self.train_inv_denominator)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T10:36:46.087360Z","iopub.status.busy":"2024-08-07T10:36:46.086770Z","iopub.status.idle":"2024-08-07T10:36:46.093169Z","shell.execute_reply":"2024-08-07T10:36:46.092057Z","shell.execute_reply.started":"2024-08-07T10:36:46.087330Z"},"trusted":true},"outputs":[],"source":["class SplineLinear(nn.Linear):\n","    def __init__(self, in_features: int, out_features: int, init_scale: float = 0.1, **kw) -> None:\n","        self.init_scale = init_scale\n","        super().__init__(in_features, out_features, bias=False, **kw)\n","\n","    def reset_parameters(self) -> None:\n","        nn.init.xavier_uniform_(self.weight)  # Using Xavier Uniform initialization"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T10:36:49.031488Z","iopub.status.busy":"2024-08-07T10:36:49.031026Z","iopub.status.idle":"2024-08-07T10:36:49.039733Z","shell.execute_reply":"2024-08-07T10:36:49.038847Z","shell.execute_reply.started":"2024-08-07T10:36:49.031456Z"},"trusted":true},"outputs":[],"source":["class FasterKANLayer(nn.Module):\n","    def __init__(\n","        self,\n","        input_dim: int,\n","        output_dim: int,\n","        grid_min: float = -1.2,\n","        grid_max: float = 0.2,\n","        num_grids: int = 8,\n","        exponent: int = 2,\n","        inv_denominator: float = 0.5,\n","        train_grid: bool = False,        \n","        train_inv_denominator: bool = False,\n","        #use_base_update: bool = True,\n","        base_activation = F.silu,\n","        spline_weight_init_scale: float = 0.667,\n","    ) -> None:\n","        super().__init__()\n","        self.layernorm = nn.LayerNorm(input_dim)\n","        self.rbf = ReflectionalSwitchFunction(grid_min, grid_max, num_grids, exponent, inv_denominator, train_grid, train_inv_denominator)\n","        self.spline_linear = SplineLinear(input_dim * num_grids, output_dim, spline_weight_init_scale)\n","\n","    def forward(self, x):\n","        #print(\"Shape before LayerNorm:\", x.shape)  # Debugging line to check the input shape\n","        x = self.layernorm(x)\n","        #print(\"Shape After LayerNorm:\", x.shape)\n","        spline_basis = self.rbf(x).view(x.shape[0], -1)\n","        ret = self.spline_linear(spline_basis)\n","        return ret"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T10:38:16.404764Z","iopub.status.busy":"2024-08-07T10:38:16.404412Z","iopub.status.idle":"2024-08-07T10:38:16.439191Z","shell.execute_reply":"2024-08-07T10:38:16.438342Z","shell.execute_reply.started":"2024-08-07T10:38:16.404737Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'cuda'"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["import numpy\n","\n","torch.manual_seed(42)\n","numpy.random.seed(42)\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","device"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T10:38:21.752190Z","iopub.status.busy":"2024-08-07T10:38:21.751593Z","iopub.status.idle":"2024-08-07T10:38:21.758420Z","shell.execute_reply":"2024-08-07T10:38:21.757449Z","shell.execute_reply.started":"2024-08-07T10:38:21.752158Z"},"trusted":true},"outputs":[],"source":["def positional_embeddings(seq_length, d):\n","    \"\"\"\n","    the purpose of this function is to find high and low interaction of a word with surrounding words.\n","    We can do so by the following equation below:\n","\n","    Args:\n","        seq_length (int): The length of the sequence/sentence\n","        d (int): The dimension of the embedding\n","    \"\"\"\n","\n","    result = torch.ones(seq_length, d)\n","    for i in range(seq_length):\n","        for j in range(d):\n","            result[i][j] = numpy.sin(i / 10000 ** (j / d)) if j % 2 == 0 else numpy.cos(i / 10000 ** (j/ d))\n","    return result"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-08-07T10:38:29.383316Z","iopub.status.busy":"2024-08-07T10:38:29.382537Z","iopub.status.idle":"2024-08-07T10:38:29.389969Z","shell.execute_reply":"2024-08-07T10:38:29.389066Z","shell.execute_reply.started":"2024-08-07T10:38:29.383285Z"},"trusted":true},"outputs":[],"source":["def patchify(images, n_patches):\n","    \"\"\"\n","    The purpose of this function is to break down the main image into multiple sub-images and map them.\n","\n","    Args:\n","        images (_type_): The image passeed into this function.\n","        n_patches (_type_): The number of sub-images that will be created.\n","    \"\"\"\n","\n","    n, c, h, w = images.shape\n","    assert h == w, \"Only for square images\"\n","\n","    patches = torch.zeros(n, n_patches**2, h * w * c // n_patches ** 2) # The equation to calculate the patches\n","    patch_size = h // n_patches\n","\n","    for idx, image in enumerate(images):\n","        for i in range(n_patches):\n","            for j in range(n_patches):\n","                patch = image[:, i * patch_size: (i + 1) * patch_size, j * patch_size: (j + 1) * patch_size]\n","                patches[idx, i * n_patches + j] = patch.flatten()\n","    return patches"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class MSA(torch.nn.Module):\n","    \"\"\"\n","        Initializes the Multi-Head Self-Attention (MSA) module with the given dimensions.\n","\n","        Args:\n","            d (int): The total dimension of the input.\n","            n_heads (int): The number of attention heads.\n","\n","        Returns:\n","            None\n","    \"\"\"\n","    def __init__(self, d, n_heads):\n","        super(MSA, self).__init__()\n","        self.d = d\n","        self.n_heads = n_heads\n","\n","        assert d % n_heads == 0\n","        d_head = int(d / n_heads)\n","\n","        self.q_mappings = torch.nn.ModuleList([FasterKANLayer(d_head, d_head) for _ in range(self.n_heads)])\n","        self.k_mappings = torch.nn.ModuleList([FasterKANLayer(d_head, d_head) for _ in range(self.n_heads)])\n","        self.v_mappings = torch.nn.ModuleList([FasterKANLayer(d_head, d_head) for _ in range(self.n_heads)])\n","        self.d_head = d_head\n","        self.softmax = torch.nn.Softmax(dim=-1)\n","\n","    def forward(self, sequence):\n","        result = []\n","        for sequence in sequence:\n","            seq_res = []\n","            for head in range(self.n_heads):\n","                q_map = self.q_mappings[head]\n","                k_map = self.k_mappings[head]\n","                v_map = self.v_mappings[head]\n","\n","                seq = sequence[:, head*self.d_head: (head+1)*self.d_head]\n","                q, k, v = q_map(seq), k_map(seq), v_map(seq)\n","\n","                attention = self.softmax(q @ k.T / (self.d_head ** 0.5))\n","                seq_res.append(attention @ v)\n","            result.append(torch.hstack(seq_res))\n","        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class FasterKAN_ViT(torch.nn.Module): \n","    \"\"\"\n","        Initializes a Vision Transformer (ViT) module.\n","\n","        Args:\n","            chw (list/tuple of 3 ints): The input image shape.\n","            n_patches (int, optional): The number of patches to split the image into. Defaults to 10.\n","            n_blocks (int, optional): The number of blocks in the transformer encoder. Defaults to 2.\n","            d_hidden (int, optional): The number of hidden dimensions in the transformer encoder. Defaults to 8.\n","            n_heads (int, optional): The number of attention heads in each block. Defaults to 2.\n","            out_d (int, optional): The number of output dimensions. Defaults to 10.\n","\n","        Returns:\n","            None\n","    \"\"\"    \n","    def __init__(self, chw, n_patches=10, n_blocks=2, d_hidden=8, n_heads=2, out_d=10): \n","        super(FasterKAN_ViT, self).__init__()\n","        \n","        self.chw = chw\n","        self.n_patches = n_patches\n","        self.n_blocks = n_blocks\n","        self.n_heads = n_heads\n","        self.d_hidden = d_hidden\n","        \n","        assert chw[1] % n_patches == 0 \n","        assert chw[2] % n_patches == 0\n","        \n","        self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n","\n","        # Linear mapping\n","        self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n","        self.linear_mapper = FasterKANLayer(self.input_d, self.d_hidden)\n","\n","        # Classification token\n","        self.v_class = torch.nn.Parameter(torch.rand(1, self.d_hidden))\n","\n","        # Positional embedding\n","        self.register_buffer('positional_embeddings', positional_embeddings(n_patches ** 2 + 1, d_hidden),\n","                             persistent=False)\n","\n","        # Encoder blocks\n","        self.blocks = torch.nn.ModuleList([MSA(d_hidden, n_heads) for _ in range(n_blocks)])\n","\n","        self.mlp = torch.nn.Sequential(\n","            FasterKANLayer(self.d_hidden, out_d),\n","            torch.nn.Softmax(dim=-1)\n","        )\n","\n","    def forward(self, images):\n","        n, c, h, w = images.shape\n","        patches = patchify(images, self.n_patches).to(self.positional_embeddings.device)\n","\n","        # running tokenization\n","        tokens = self.linear_mapper(patches)\n","        tokens = torch.cat((self.v_class.expand(n, 1, -1), tokens), dim=1)\n","        out = tokens + self.positional_embeddings.repeat(n, 1, 1)\n","\n","        for block in self.blocks:\n","            out = block(out)\n","\n","        out = out[:, 0]\n","        return self.mlp(out)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","mnist_model = FasterKAN_ViT((1, 28, 28), n_patches=7, n_blocks=2, d_hidden=8, n_heads=2, out_d=10).to(device)\n","optimizer = Adam(mnist_model.parameters(), lr=0.005)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torch.optim import Adam\n","from torch.utils.data import DataLoader\n","from torchvision import transforms\n","from torchvision.datasets import MNIST\n","from tqdm import tqdm, trange\n","\n","def main(train_loader, test_loader):\n","    \"\"\"\n","    This code contains the training and testing loop for training the vision transformers model. It requires two\n","    parameters\n","\n","    :param train_loader: The dataloader for the training set for training the model.\n","    :param test_loader: The dataloader for the testing set during evaluation phase.\n","    \"\"\"\n","    print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n","\n","    epochs = 10\n","    criterion = torch.nn.CrossEntropyLoss()\n","    for epoch in trange(epochs, desc=\"train\"):\n","        train_loss = 0.0\n","        for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1} in training\", leave=False):\n","            x, y = batch\n","            x, y = x.to(device), y.to(device)\n","            y_hat = mnist_model(x)\n","            loss = criterion(y_hat, y)\n","\n","            train_loss += loss.detach().cpu().item() / len(train_loader)\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","        print(f\"Epoch {epoch + 1}/{epochs} loss: {train_loss:.2f}\")\n","\n","    with torch.no_grad():\n","        correct, total = 0, 0\n","        test_loss = 0.0\n","        for batch in tqdm(test_loader, desc=\"Testing\"):\n","            x, y = batch\n","            x, y = x.to(device), y.to(device)\n","            y_hat = mnist_model(x)\n","            loss = criterion(y_hat, y)\n","            test_loss += loss.detach().cpu().item() / len(test_loader)\n","\n","            correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n","            total += len(x)\n","\n","        print(f\"Test loss: {test_loss:.2f}\")\n","        print(f\"Test accuracy: {correct / total * 100:.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["transform = transforms.ToTensor()\n","train_mnist = MNIST(root='./mnist', train=True, download=True, transform=transform)\n","test_mnist = MNIST(root='./mnist', train=False, download=True, transform=transform)\n","train_loader = DataLoader(train_mnist, shuffle=True, batch_size=128)\n","test_loader = DataLoader(test_mnist, shuffle=False, batch_size=128)\n","main(train_loader=train_loader, test_loader=test_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["path: str = \"fasterkan_vit_10epochs.pth\"\n","\n","torch.save(mnist_model.state_dict(), path)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}
